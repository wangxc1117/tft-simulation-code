{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d3dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded spatial_utils from: /home/wangxc1117/geospatial-neural-adapter/geospatial_neural_adapter/cpp_extensions/spatial_utils.so\n",
      "Using CUDA: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   Memory: 8.6 GB\n",
      "\n",
      "=== OFFICIAL (paper-like) window ===\n",
      "Train: 2015-01-01 → 2015-12-01\n",
      "Val  : 2015-12-02 → 2015-12-31\n",
      "Test : 2016-01-01 → 2016-01-30\n",
      "All  : 2015-01-01 → 2016-01-30\n",
      "\n",
      "=== Loading train.csv ===\n",
      "Paper-window date range (in df): 2015-01-01 00:00:00 → 2016-01-30 00:00:00\n",
      "\n",
      "Using ALL 53 stores in paper window.\n",
      "T_all (days): 395\n",
      "\n",
      "=== Target sanity ===\n",
      "targets_full: (395, 53) | has_na: False\n",
      "\n",
      "=== OFFICIAL split lengths ===\n",
      "T_all: 395 | train: 335 | val: 30 | test: 30\n",
      "\n",
      "=== OLS data (raw, before scaling) ===\n",
      "X_raw: (394, 53, 1) | y_raw: (394, 53) | p_dim: 1\n",
      "\n",
      "=== SHIFTED indices (for lag1 model) ===\n",
      "T (shifted)     = 394\n",
      "train_len       = 334\n",
      "val_start       = 334 | val_end(excl) = 364\n",
      "test_start      = 364 | test_end(excl)= 394\n",
      "\n",
      "=== After scaling ===\n",
      "X_s: (394, 53, 1) | y_s: (394, 53)\n",
      "[NaN/inf check after scaling]\n",
      "X_s has NaN: False | inf: False\n",
      "y_s has NaN: False | inf: False\n",
      "\n",
      "=== Feature variance screening (TRAIN ONLY, pooled) ===\n",
      "EPS_STD = 1e-08\n",
      "All feature std (TRAIN):\n",
      "  [00] lag1         std=9.999999403954e-01\n",
      "\n",
      "Dropped near-constant features: NONE\n",
      "\n",
      "Kept features:\n",
      "  KEEP [00] lag1  std=9.999999403954e-01\n",
      "\n",
      "=== Reduced feature matrix ===\n",
      "X_s_red: (394, 53, 1) | p_red: 1\n",
      "feat_names_red: ['lag1']\n",
      "\n",
      "=== OLS fitted (reduced) ===\n",
      "beta_red shape: (1,) | b0: 1.6247816159413242e-09\n",
      "beta_red finite: True | b0 finite: True\n",
      "\n",
      "=== OLS coefficients mapped back to FULL feature space ===\n",
      "beta_full shape: (1,)\n",
      "beta_full finite: True\n",
      "\n",
      "=== Feature index mapping (full -> reduced) ===\n",
      "lag1: full = 0 -> red = 0\n",
      "\n",
      "=== OLS (LAG1 ONLY; no cov) OFFICIAL eval (standardized space) ===\n",
      "Val  RMSE=0.270079, MAE=0.209705, R2=-5.134000\n",
      "Test RMSE=0.229836, MAE=0.175023, R2=-2.592144\n",
      "\n",
      "=== P50 q-risk (Val) ===\n",
      "qrisk_p50_scaled_log   = 0.539691\n",
      "qrisk_p50_unscaled_log = 0.044964\n",
      "qrisk_p50_sales        = 0.415516\n",
      "\n",
      "=== P50 q-risk (Test) ===\n",
      "qrisk_p50_scaled_log   = 0.522304\n",
      "qrisk_p50_unscaled_log = 0.038134\n",
      "qrisk_p50_sales        = 0.362250\n",
      "\n",
      "Start plotting ALL 53 stores (2 figs/store) ...\n",
      "All figures saved under: OLS_plots_store_paperWindow_h30_LAG1_ONLY\n",
      "\n",
      "Saved metrics to metrics_summary_OLS_Favorita_STORE_H30_paperWindow_LAG1_ONLY.csv\n",
      "Saved OLS plots to folder: OLS_plots_store_paperWindow_h30_LAG1_ONLY\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    get_device_info,\n",
    "    compute_ols_coefficients,\n",
    ")\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling\n",
    "\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info[\"device\"] == \"cuda\":\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "H = 30\n",
    "\n",
    "PAPER_TRAIN_START = pd.Timestamp(\"2015-01-01\")\n",
    "PAPER_TRAIN_END = pd.Timestamp(\"2015-12-01\")\n",
    "PAPER_VAL_DAYS = 30\n",
    "PAPER_TEST_DAYS = 30\n",
    "\n",
    "PAPER_VAL_START = PAPER_TRAIN_END + pd.Timedelta(days=1)\n",
    "PAPER_VAL_END = PAPER_VAL_START + pd.Timedelta(days=PAPER_VAL_DAYS - 1)\n",
    "PAPER_TEST_START = PAPER_VAL_END + pd.Timedelta(days=1)\n",
    "PAPER_TEST_END = PAPER_TEST_START + pd.Timedelta(days=PAPER_TEST_DAYS - 1)\n",
    "\n",
    "PAPER_ALL_START = PAPER_TRAIN_START\n",
    "PAPER_ALL_END = PAPER_TEST_END\n",
    "\n",
    "print(\"\\n=== OFFICIAL (paper-like) window ===\")\n",
    "print(\"Train:\", PAPER_TRAIN_START.date(), \"→\", PAPER_TRAIN_END.date())\n",
    "print(\"Val  :\", PAPER_VAL_START.date(), \"→\", PAPER_VAL_END.date())\n",
    "print(\"Test :\", PAPER_TEST_START.date(), \"→\", PAPER_TEST_END.date())\n",
    "print(\"All  :\", PAPER_ALL_START.date(), \"→\", PAPER_ALL_END.date())\n",
    "\n",
    "\n",
    "def quantile_risk(y_true: np.ndarray, y_pred: np.ndarray, q: float = 0.5) -> float:\n",
    "    y = np.asarray(y_true, dtype=np.float64).reshape(-1)\n",
    "    yhat = np.asarray(y_pred, dtype=np.float64).reshape(-1)\n",
    "\n",
    "    diff = y - yhat\n",
    "    pinball = np.maximum(q * diff, (q - 1.0) * diff)\n",
    "    denom = np.sum(np.abs(y))\n",
    "\n",
    "    if denom <= 0:\n",
    "        return float(\"nan\")\n",
    "    return float(2.0 * np.sum(pinball) / denom)\n",
    "\n",
    "\n",
    "def inverse_standardize_targets(preprocessor: Any, y_scaled: np.ndarray) -> np.ndarray:\n",
    "    y_scaled = np.asarray(y_scaled, dtype=np.float32)\n",
    "    scaler = None\n",
    "\n",
    "    for name in [\"target_scaler\", \"target_scaler_\", \"y_scaler\", \"y_scaler_\"]:\n",
    "        if hasattr(preprocessor, name):\n",
    "            scaler = getattr(preprocessor, name)\n",
    "            if scaler is not None:\n",
    "                break\n",
    "\n",
    "    if scaler is None:\n",
    "        raise AttributeError(\n",
    "            \"preprocessor does not expose a target scaler attribute (target_scaler/target_scaler_). \"\n",
    "            \"Please check prepare_all_with_scaling() return object and adjust inverse_standardize_targets().\"\n",
    "        )\n",
    "\n",
    "    if hasattr(scaler, \"mean_\") and hasattr(scaler, \"scale_\"):\n",
    "        mean_ = np.asarray(scaler.mean_, dtype=np.float32)\n",
    "        scale_ = np.asarray(scaler.scale_, dtype=np.float32)\n",
    "\n",
    "        if mean_.ndim == 0:\n",
    "            mean_ = mean_.reshape(1)\n",
    "        if scale_.ndim == 0:\n",
    "            scale_ = scale_.reshape(1)\n",
    "\n",
    "        if mean_.shape[0] == 1:\n",
    "            mean_b = mean_[0]\n",
    "        else:\n",
    "            mean_b = mean_.reshape(1, -1)\n",
    "\n",
    "        if scale_.shape[0] == 1:\n",
    "            scale_b = scale_[0]\n",
    "        else:\n",
    "            scale_b = scale_.reshape(1, -1)\n",
    "\n",
    "        return y_scaled * scale_b + mean_b\n",
    "\n",
    "    raise AttributeError(\n",
    "        \"Found a target scaler object but it does not have mean_ and scale_. \"\n",
    "        \"Please adapt inverse_standardize_targets() to your scaler type.\"\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_ROOT = Path(\"/home/wangxc1117/experiment_data/sales_forecasting_data\")\n",
    "TRAIN_PATH = DATA_ROOT / \"train.csv\"\n",
    "\n",
    "if not TRAIN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"train.csv not found at {TRAIN_PATH}\")\n",
    "\n",
    "print(\"\\n=== Loading train.csv ===\")\n",
    "df = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"onpromotion\"] = df[\"onpromotion\"].fillna(0).astype(int)\n",
    "\n",
    "df = df[(df[\"date\"] >= PAPER_ALL_START) & (df[\"date\"] <= PAPER_ALL_END)].copy()\n",
    "print(f\"Paper-window date range (in df): {df['date'].min()} → {df['date'].max()}\")\n",
    "\n",
    "df_store_sales = (\n",
    "    df.groupby([\"date\", \"store_nbr\"], as_index=False)[\"unit_sales\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"unit_sales\": \"store_sales\"})\n",
    ")\n",
    "df_store_sales[\"store_sales\"] = df_store_sales[\"store_sales\"].clip(lower=0.0)\n",
    "\n",
    "stores = sorted(df_store_sales[\"store_nbr\"].unique())\n",
    "date_index = pd.date_range(start=PAPER_ALL_START, end=PAPER_ALL_END, freq=\"D\")\n",
    "T_all = len(date_index)\n",
    "N = len(stores)\n",
    "print(f\"\\nUsing ALL {N} stores in paper window.\")\n",
    "print(f\"T_all (days): {T_all}\")\n",
    "\n",
    "full_idx = pd.MultiIndex.from_product([date_index, stores], names=[\"date\", \"store_nbr\"])\n",
    "\n",
    "panel_sales = (\n",
    "    df_store_sales.set_index([\"date\", \"store_nbr\"])\n",
    "    .reindex(full_idx)\n",
    "    .sort_index()\n",
    ")\n",
    "panel_sales[\"store_sales\"] = panel_sales.groupby(\"store_nbr\")[\"store_sales\"].ffill()\n",
    "panel_sales.loc[panel_sales[\"store_sales\"].isna(), \"store_sales\"] = 0.0\n",
    "panel_sales[\"log_sales\"] = np.log1p(panel_sales[\"store_sales\"]).astype(\"float32\")\n",
    "\n",
    "Y_df = (\n",
    "    panel_sales[\"log_sales\"]\n",
    "    .unstack(\"store_nbr\")\n",
    "    .reindex(index=date_index, columns=stores)\n",
    "    .astype(\"float32\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Target sanity ===\")\n",
    "print(\"targets_full:\", Y_df.to_numpy(dtype=np.float32).shape, \"| has_na:\", bool(np.isnan(Y_df.to_numpy()).any()))\n",
    "\n",
    "cut_train = int((PAPER_TRAIN_END - PAPER_ALL_START).days + 1)\n",
    "cut_val = cut_train + PAPER_VAL_DAYS\n",
    "assert cut_val + PAPER_TEST_DAYS == T_all\n",
    "\n",
    "print(\"\\n=== OFFICIAL split lengths ===\")\n",
    "print(\"T_all:\", T_all, \"| train:\", cut_train, \"| val:\", (cut_val - cut_train), \"| test:\", (T_all - cut_val))\n",
    "\n",
    "lag1_df = Y_df.shift(1)\n",
    "\n",
    "Y_shift = Y_df.iloc[1:, :].to_numpy(dtype=np.float32)\n",
    "lag1_shift = lag1_df.iloc[1:, :].to_numpy(dtype=np.float32)\n",
    "\n",
    "T = Y_shift.shape[0]\n",
    "assert T == T_all - 1\n",
    "\n",
    "feat_names_full = [\"lag1\"]\n",
    "\n",
    "X_raw = lag1_shift[..., None].astype(\"float32\")\n",
    "y_raw = Y_shift.astype(\"float32\")\n",
    "\n",
    "p_dim = X_raw.shape[2]\n",
    "print(\"\\n=== OLS data (raw, before scaling) ===\")\n",
    "print(\"X_raw:\", X_raw.shape, \"| y_raw:\", y_raw.shape, \"| p_dim:\", p_dim)\n",
    "\n",
    "if np.isnan(X_raw).any():\n",
    "    raise ValueError(\"NaN found in X_raw. Check lag1 construction.\")\n",
    "if np.isnan(y_raw).any():\n",
    "    raise ValueError(\"NaN found in y_raw. Check target construction.\")\n",
    "\n",
    "train_len = cut_train - 1\n",
    "val_len = PAPER_VAL_DAYS\n",
    "test_len = PAPER_TEST_DAYS\n",
    "\n",
    "val_start = train_len\n",
    "test_start = (cut_val - 1)\n",
    "assert val_start + val_len == test_start\n",
    "assert test_start + test_len == T\n",
    "\n",
    "print(\"\\n=== SHIFTED indices (for lag1 model) ===\")\n",
    "print(\"T (shifted)     =\", T)\n",
    "print(\"train_len       =\", train_len)\n",
    "print(\"val_start       =\", val_start, \"| val_end(excl) =\", val_start + val_len)\n",
    "print(\"test_start      =\", test_start, \"| test_end(excl)=\", test_start + test_len)\n",
    "\n",
    "cat_dummy = np.zeros((T, N, 1), dtype=np.int64)\n",
    "train_ratio = train_len / T\n",
    "val_ratio = val_len / T\n",
    "\n",
    "train_ds, val_ds, test_ds, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_dummy,\n",
    "    cont_features=X_raw,\n",
    "    targets=y_raw,\n",
    "    train_ratio=train_ratio,\n",
    "    val_ratio=val_ratio,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def _stitch_x(dsets) -> np.ndarray:\n",
    "    xs = [ds.tensors[1].cpu().numpy().astype(np.float32) for ds in dsets]\n",
    "    return np.concatenate(xs, axis=0)\n",
    "\n",
    "\n",
    "def _stitch_y(dsets) -> np.ndarray:\n",
    "    ys = [ds.tensors[2].cpu().numpy().astype(np.float32) for ds in dsets]\n",
    "    return np.concatenate(ys, axis=0)\n",
    "\n",
    "\n",
    "X_s = _stitch_x([train_ds, val_ds, test_ds])\n",
    "y_s = _stitch_y([train_ds, val_ds, test_ds])\n",
    "\n",
    "print(\"\\n=== After scaling ===\")\n",
    "print(\"X_s:\", X_s.shape, \"| y_s:\", y_s.shape)\n",
    "print(\"[NaN/inf check after scaling]\")\n",
    "print(\"X_s has NaN:\", bool(np.isnan(X_s).any()), \"| inf:\", bool(np.isinf(X_s).any()))\n",
    "print(\"y_s has NaN:\", bool(np.isnan(y_s).any()), \"| inf:\", bool(np.isinf(y_s).any()))\n",
    "\n",
    "if (not np.isfinite(X_s).all()) or (not np.isfinite(y_s).all()):\n",
    "    raise ValueError(\"Scaling produced non-finite values. Stop.\")\n",
    "\n",
    "EPS_STD = 1e-8\n",
    "\n",
    "X_train_s = X_s[:train_len, :, :]\n",
    "X_flat_train = X_train_s.reshape(-1, p_dim)\n",
    "\n",
    "feat_std = X_flat_train.std(axis=0)\n",
    "\n",
    "keep_mask = feat_std > EPS_STD\n",
    "drop_mask = ~keep_mask\n",
    "\n",
    "kept_idx = np.where(keep_mask)[0].tolist()\n",
    "dropped_idx = np.where(drop_mask)[0].tolist()\n",
    "\n",
    "print(\"\\n=== Feature variance screening (TRAIN ONLY, pooled) ===\")\n",
    "print(f\"EPS_STD = {EPS_STD}\")\n",
    "print(\"All feature std (TRAIN):\")\n",
    "for i, nm in enumerate(feat_names_full):\n",
    "    print(f\"  [{i:02d}] {nm:12s} std={feat_std[i]:.12e}\")\n",
    "\n",
    "if dropped_idx:\n",
    "    print(\"\\nDropped near-constant features (std <= EPS_STD):\")\n",
    "    for i in dropped_idx:\n",
    "        print(f\"  DROP [{i:02d}] {feat_names_full[i]}  std={feat_std[i]:.12e}\")\n",
    "else:\n",
    "    print(\"\\nDropped near-constant features: NONE\")\n",
    "\n",
    "print(\"\\nKept features:\")\n",
    "for i in kept_idx:\n",
    "    print(f\"  KEEP [{i:02d}] {feat_names_full[i]}  std={feat_std[i]:.12e}\")\n",
    "\n",
    "if len(kept_idx) == 0:\n",
    "    raise ValueError(\"All features were dropped as near-constant. Cannot fit OLS.\")\n",
    "\n",
    "X_s_red = X_s[:, :, keep_mask].astype(np.float32)\n",
    "p_red = X_s_red.shape[2]\n",
    "feat_names_red = [feat_names_full[i] for i in kept_idx]\n",
    "\n",
    "print(\"\\n=== Reduced feature matrix ===\")\n",
    "print(\"X_s_red:\", X_s_red.shape, \"| p_red:\", p_red)\n",
    "print(\"feat_names_red:\", feat_names_red)\n",
    "\n",
    "X_train_red_t = torch.from_numpy(X_s_red[:train_len]).to(DEVICE)\n",
    "y_train_t = torch.from_numpy(y_s[:train_len]).to(DEVICE)\n",
    "\n",
    "beta_red_t, b0 = compute_ols_coefficients(X_train_red_t, y_train_t, device=DEVICE)\n",
    "beta_red = np.asarray(beta_red_t.detach().cpu().numpy(), dtype=np.float32).reshape(-1)\n",
    "b0 = float(b0)\n",
    "\n",
    "beta_full = np.zeros((p_dim,), dtype=np.float32)\n",
    "beta_full[keep_mask] = beta_red\n",
    "\n",
    "print(\"\\n=== OLS fitted (reduced) ===\")\n",
    "print(\"beta_red shape:\", beta_red.shape, \"| b0:\", b0)\n",
    "print(\"beta_red finite:\", bool(np.isfinite(beta_red).all()), \"| b0 finite:\", bool(np.isfinite(b0)))\n",
    "\n",
    "print(\"\\n=== OLS coefficients mapped back to FULL feature space ===\")\n",
    "print(\"beta_full shape:\", beta_full.shape)\n",
    "print(\"beta_full finite:\", bool(np.isfinite(beta_full).all()))\n",
    "\n",
    "if not np.isfinite(beta_red).all():\n",
    "    raise ValueError(\"beta_red is non-finite. Consider CPU solve or ridge.\")\n",
    "\n",
    "IDX_LAG1_FULL = 0\n",
    "\n",
    "\n",
    "def _map_full_idx_to_red(full_idx: int, keep_mask_arr: np.ndarray) -> int:\n",
    "    if not keep_mask_arr[full_idx]:\n",
    "        return -1\n",
    "    return int(np.sum(keep_mask_arr[:full_idx]))\n",
    "\n",
    "\n",
    "IDX_LAG1_RED = _map_full_idx_to_red(IDX_LAG1_FULL, keep_mask)\n",
    "\n",
    "print(\"\\n=== Feature index mapping (full -> reduced) ===\")\n",
    "print(\"lag1: full =\", IDX_LAG1_FULL, \"-> red =\", IDX_LAG1_RED)\n",
    "\n",
    "if IDX_LAG1_RED < 0:\n",
    "    raise ValueError(\"lag1 was dropped as near-constant, which should not happen. Check your data.\")\n",
    "\n",
    "\n",
    "def ols_direct_h_lag1_only(\n",
    "    beta_red: np.ndarray,\n",
    "    b0: float,\n",
    "    X_s_red_full: np.ndarray,\n",
    "    y_s_full: np.ndarray,\n",
    "    start_idx: int,\n",
    "    horizon: int = 30,\n",
    ") -> np.ndarray:\n",
    "    beta_red = np.asarray(beta_red, dtype=np.float32).reshape(-1)\n",
    "    T_tot, N_loc, p = X_s_red_full.shape\n",
    "    assert p == beta_red.shape[0]\n",
    "\n",
    "    end_idx = start_idx + horizon\n",
    "    if start_idx < 1:\n",
    "        raise ValueError(\"start_idx must be >=1 for lag1 anchor.\")\n",
    "    if end_idx > T_tot:\n",
    "        raise ValueError(f\"Need end_idx={end_idx} <= T_tot={T_tot}\")\n",
    "\n",
    "    yhat = np.empty((horizon, N_loc), dtype=np.float32)\n",
    "\n",
    "    prev = y_s_full[start_idx - 1, :].astype(np.float32)\n",
    "\n",
    "    for k, t in enumerate(range(start_idx, end_idx)):\n",
    "        Xt = X_s_red_full[t, :, :].copy()\n",
    "        Xt[:, IDX_LAG1_RED] = prev\n",
    "\n",
    "        y_pred = (Xt @ beta_red.reshape(p, 1)).reshape(-1) + float(b0)\n",
    "\n",
    "        if not np.isfinite(y_pred).all():\n",
    "            bad = ~np.isfinite(y_pred)\n",
    "            idx = int(np.where(bad)[0][0])\n",
    "            print(\"\\n[ERROR] non-finite in y_pred at step k =\", k, \" store_idx =\", idx, \" t =\", t)\n",
    "            print(\"  y_pred[idx] =\", y_pred[idx])\n",
    "            print(\"  prev[idx]   =\", prev[idx])\n",
    "            print(\"  Xt[idx,:] finite:\", bool(np.isfinite(Xt[idx, :]).all()))\n",
    "            print(\"  beta_red finite:\", bool(np.isfinite(beta_red).all()), \"| b0 finite:\", bool(np.isfinite(b0)))\n",
    "            raise ValueError(\"y_pred contains NaN/inf (likely numerical instability).\")\n",
    "\n",
    "        yhat[k, :] = y_pred.astype(np.float32)\n",
    "        prev = y_pred.astype(np.float32)\n",
    "\n",
    "    return yhat\n",
    "\n",
    "\n",
    "yhat_val_s = ols_direct_h_lag1_only(\n",
    "    beta_red=beta_red,\n",
    "    b0=b0,\n",
    "    X_s_red_full=X_s_red,\n",
    "    y_s_full=y_s,\n",
    "    start_idx=val_start,\n",
    "    horizon=H,\n",
    ")\n",
    "\n",
    "yhat_test_s = ols_direct_h_lag1_only(\n",
    "    beta_red=beta_red,\n",
    "    b0=b0,\n",
    "    X_s_red_full=X_s_red,\n",
    "    y_s_full=y_s,\n",
    "    start_idx=test_start,\n",
    "    horizon=H,\n",
    ")\n",
    "\n",
    "y_val_true_s = y_s[val_start:val_start + H, :]\n",
    "y_test_true_s = y_s[test_start:test_start + H, :]\n",
    "\n",
    "val_y_t = torch.from_numpy(y_val_true_s).to(DEVICE)\n",
    "test_y_t = torch.from_numpy(y_test_true_s).to(DEVICE)\n",
    "yhat_val_t = torch.from_numpy(yhat_val_s).to(DEVICE)\n",
    "yhat_test_t = torch.from_numpy(yhat_test_s).to(DEVICE)\n",
    "\n",
    "rmse_v, mae_v, r2_v = compute_metrics(val_y_t, yhat_val_t)\n",
    "rmse_t, mae_t, r2_t = compute_metrics(test_y_t, yhat_test_t)\n",
    "\n",
    "print(\"\\n=== OLS (LAG1 ONLY; no cov) OFFICIAL eval (standardized space) ===\")\n",
    "print(f\"Val  RMSE={rmse_v:.6f}, MAE={mae_v:.6f}, R2={r2_v:.6f}\")\n",
    "print(f\"Test RMSE={rmse_t:.6f}, MAE={mae_t:.6f}, R2={r2_t:.6f}\")\n",
    "\n",
    "qrisk_val_scaled_log = quantile_risk(y_val_true_s, yhat_val_s, q=0.5)\n",
    "qrisk_test_scaled_log = quantile_risk(y_test_true_s, yhat_test_s, q=0.5)\n",
    "\n",
    "y_val_true_log = inverse_standardize_targets(preprocessor, y_val_true_s)\n",
    "yhat_val_log = inverse_standardize_targets(preprocessor, yhat_val_s)\n",
    "y_test_true_log = inverse_standardize_targets(preprocessor, y_test_true_s)\n",
    "yhat_test_log = inverse_standardize_targets(preprocessor, yhat_test_s)\n",
    "\n",
    "qrisk_val_unscaled_log = quantile_risk(y_val_true_log, yhat_val_log, q=0.5)\n",
    "qrisk_test_unscaled_log = quantile_risk(y_test_true_log, yhat_test_log, q=0.5)\n",
    "\n",
    "y_val_true_sales = np.expm1(y_val_true_log).astype(np.float64)\n",
    "yhat_val_sales = np.expm1(yhat_val_log).astype(np.float64)\n",
    "y_test_true_sales = np.expm1(y_test_true_log).astype(np.float64)\n",
    "yhat_test_sales = np.expm1(yhat_test_log).astype(np.float64)\n",
    "\n",
    "y_val_true_sales = np.clip(y_val_true_sales, 0.0, None)\n",
    "yhat_val_sales = np.clip(yhat_val_sales, 0.0, None)\n",
    "y_test_true_sales = np.clip(y_test_true_sales, 0.0, None)\n",
    "yhat_test_sales = np.clip(yhat_test_sales, 0.0, None)\n",
    "\n",
    "qrisk_val_sales = quantile_risk(y_val_true_sales, yhat_val_sales, q=0.5)\n",
    "qrisk_test_sales = quantile_risk(y_test_true_sales, yhat_test_sales, q=0.5)\n",
    "\n",
    "print(\"\\n=== P50 q-risk (Val) ===\")\n",
    "print(f\"qrisk_p50_scaled_log   = {qrisk_val_scaled_log:.6f}\")\n",
    "print(f\"qrisk_p50_unscaled_log = {qrisk_val_unscaled_log:.6f}\")\n",
    "print(f\"qrisk_p50_sales        = {qrisk_val_sales:.6f}\")\n",
    "\n",
    "print(\"\\n=== P50 q-risk (Test) ===\")\n",
    "print(f\"qrisk_p50_scaled_log   = {qrisk_test_scaled_log:.6f}\")\n",
    "print(f\"qrisk_p50_unscaled_log = {qrisk_test_unscaled_log:.6f}\")\n",
    "print(f\"qrisk_p50_sales        = {qrisk_test_sales:.6f}\")\n",
    "\n",
    "OLS_PLOTS_DIR = Path(f\"OLS_plots_store_paperWindow_h{H}_LAG1_ONLY\")\n",
    "OLS_PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "date_index_shift = date_index[1:]\n",
    "assert len(date_index_shift) == y_s.shape[0] == X_s.shape[0]\n",
    "\n",
    "\n",
    "def plot_store_two_figs_like_tft_ols(\n",
    "    sid: int,\n",
    "    y_true_scaled_full_shift: np.ndarray,\n",
    "    yhat_test_scaled: np.ndarray,\n",
    "    out_dir: Path,\n",
    "):\n",
    "    j = stores.index(sid)\n",
    "\n",
    "    y_val_true = y_true_scaled_full_shift[val_start:val_start + H, j]\n",
    "    y_test_true = y_true_scaled_full_shift[test_start:test_start + H, j]\n",
    "    y_test_pred = yhat_test_scaled[:, j]\n",
    "\n",
    "    dates_val = date_index_shift[val_start:val_start + H]\n",
    "    dates_test = date_index_shift[test_start:test_start + H]\n",
    "\n",
    "    dates_true_vt = dates_val.append(dates_test)\n",
    "    y_true_vt = np.concatenate([y_val_true, y_test_true], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=140)\n",
    "    plt.plot(dates_true_vt, y_true_vt, \"-\", linewidth=2.0, color=\"k\", label=\"True (Val+Test)\")\n",
    "    plt.plot(dates_test, y_test_pred, \"-\", linewidth=1.8, color=\"C3\", label=\"Test Pred\")\n",
    "    plt.axvline(PAPER_VAL_START, linestyle=\"--\", linewidth=1, label=\"train/val split\")\n",
    "    plt.axvline(PAPER_TEST_START, linestyle=\"--\", linewidth=1, label=\"val/test split\")\n",
    "    plt.title(f\"FIG1 H{H} store {sid}: True (continuous) + Test Pred (OLS LAG1 ONLY)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Scaled log_sales\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    p1 = out_dir / f\"FIG1_store{sid}_h{H}_ols_lag1only.png\"\n",
    "    plt.savefig(p1)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=140)\n",
    "    y_all_true = y_true_scaled_full_shift[:, j]\n",
    "    plt.plot(date_index_shift, y_all_true, \"-\", linewidth=1.8, color=\"k\", label=\"All True\")\n",
    "    plt.plot(dates_test, y_test_pred, \"-\", linewidth=1.8, color=\"C3\", label=\"Test Pred\")\n",
    "    plt.axvline(PAPER_VAL_START, linestyle=\"--\", linewidth=1, label=\"train/val split\")\n",
    "    plt.axvline(PAPER_TEST_START, linestyle=\"--\", linewidth=1, label=\"val/test split\")\n",
    "    plt.title(f\"FIG2 H{H} store {sid}: All True + Test Pred (OLS LAG1 ONLY)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Scaled log_sales\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    p2 = out_dir / f\"FIG2_store{sid}_h{H}_ols_lag1only.png\"\n",
    "    plt.savefig(p2)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_all_stores_two_figs_like_tft_ols(\n",
    "    y_true_scaled_full_shift: np.ndarray,\n",
    "    yhat_test_scaled: np.ndarray,\n",
    "):\n",
    "    print(f\"\\nStart plotting ALL {len(stores)} stores (2 figs/store) ...\")\n",
    "    for sid in stores:\n",
    "        plot_store_two_figs_like_tft_ols(\n",
    "            sid=sid,\n",
    "            y_true_scaled_full_shift=y_true_scaled_full_shift,\n",
    "            yhat_test_scaled=yhat_test_scaled,\n",
    "            out_dir=OLS_PLOTS_DIR,\n",
    "        )\n",
    "    print(f\"All figures saved under: {OLS_PLOTS_DIR}\")\n",
    "\n",
    "\n",
    "plot_all_stores_two_figs_like_tft_ols(\n",
    "    y_true_scaled_full_shift=y_s,\n",
    "    yhat_test_scaled=yhat_test_s,\n",
    ")\n",
    "\n",
    "csv_path = Path(f\"metrics_summary_OLS_Favorita_STORE_H{H}_paperWindow_LAG1_ONLY.csv\")\n",
    "write_header = not csv_path.exists()\n",
    "\n",
    "notes = (\n",
    "    \"X_full=[lag1 only]; \"\n",
    "    f\"drop_near_constant(EPS_STD={EPS_STD}) on TRAIN ONLY; \"\n",
    "    \"direct 30-step; paper window; no covariates; plots like TFT (2 figs/store)\"\n",
    ")\n",
    "\n",
    "with csv_path.open(\"a\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    if write_header:\n",
    "        w.writerow([\n",
    "            \"seed\", \"model\",\n",
    "            \"rmse_val\", \"mae_val\", \"r2_val\",\n",
    "            \"rmse_test\", \"mae_test\", \"r2_test\",\n",
    "            \"qrisk_p50_scaled_log_val\", \"qrisk_p50_scaled_log_test\",\n",
    "            \"qrisk_p50_unscaled_log_val\", \"qrisk_p50_unscaled_log_test\",\n",
    "            \"qrisk_p50_sales_val\", \"qrisk_p50_sales_test\",\n",
    "            \"dropped_features\",\n",
    "            \"kept_features\",\n",
    "            \"notes\",\n",
    "        ])\n",
    "    w.writerow([\n",
    "        GLOBAL_SEED,\n",
    "        f\"OLS_STORE_H{H}_paperWindow_LAG1_ONLY\",\n",
    "        float(rmse_v), float(mae_v), float(r2_v),\n",
    "        float(rmse_t), float(mae_t), float(r2_t),\n",
    "        float(qrisk_val_scaled_log), float(qrisk_test_scaled_log),\n",
    "        float(qrisk_val_unscaled_log), float(qrisk_test_unscaled_log),\n",
    "        float(qrisk_val_sales), float(qrisk_test_sales),\n",
    "        \";\".join([feat_names_full[i] for i in dropped_idx]) if dropped_idx else \"\",\n",
    "        \";\".join(feat_names_red),\n",
    "        notes,\n",
    "    ])\n",
    "\n",
    "print(f\"\\nSaved metrics to {csv_path}\")\n",
    "print(f\"Saved OLS plots to folder: {OLS_PLOTS_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
