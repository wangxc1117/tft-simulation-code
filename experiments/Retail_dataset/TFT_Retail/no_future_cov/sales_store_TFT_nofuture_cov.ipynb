{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangxc1117/miniconda3/envs/geospatial-neural-adapter/lib/python3.10/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded spatial_utils from: /home/wangxc1117/geospatial-neural-adapter/geospatial_neural_adapter/cpp_extensions/spatial_utils.so\n",
      "Using CUDA: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   Memory: 8.6 GB\n",
      "\n",
      "=== TFT (STORE-level): input=90, output=30 (30-step direct predict) ===\n",
      "Config: add_relative_index=True (only time feature).\n",
      "Future covariates: NONE\n",
      "Past covariates  : oil_std ONLY\n",
      "Static covariates: one-hot store meta\n",
      "OFFICIAL inference: BATCH predict\n",
      "oil standardized using TRAIN-only mean/std\n",
      "\n",
      "=== OFFICIAL (paper-like) window (for reporting/eval only) ===\n",
      "Train: 2015-01-01 → 2015-12-01\n",
      "Val  : 2015-12-02 → 2015-12-31\n",
      "Test : 2016-01-01 → 2016-01-30\n",
      "All  : 2015-01-01 → 2016-01-30\n",
      "\n",
      "=== Loading train.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1512668/2997895316.py:156: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(TRAIN_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper-window date range (in df): 2015-01-01 00:00:00 → 2016-01-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ALL 53 stores in paper window.\n",
      "T_all (days): 395\n",
      "\n",
      "=== Target sanity ===\n",
      "targets_full shape: (395, 53)\n",
      "targets_full has_na: False\n",
      "\n",
      "Static cov (one-hot) shape: (53, 60)\n",
      "Oil covariate: has_na = False\n",
      "Transactions loaded (NOT used): has_na = False\n",
      "\n",
      "=== OFFICIAL split lengths (reporting only) ===\n",
      "T_all: 395 | train: 335 | val: 30 | test: 30\n",
      "y_all_s shape: (395, 53)\n",
      "\n",
      "=== TRAIN-only standardization stats ===\n",
      "oil: mean=49.765164, std=6.051614\n",
      "\n",
      "Built 53 TimeSeries. Future covariates: NONE. Past cov: oil_std only.\n",
      "\n",
      "=== INTERNAL validation (for early stopping only) ===\n",
      "Train idx: [0 : 335 ) len = 335\n",
      "IntVal idx: [ 95 : 335 ) len = 240\n",
      "IntVal covers dates: 2015-04-06 → 2015-12-01\n",
      "Experiment Configuration:\n",
      "  Trials per seed: 1\n",
      "  Dataset seeds: 1 to 1\n",
      "  Total experiments: 1\n",
      "  Device: GPU\n",
      "\n",
      "Starting TFT STORE-level H30 training for seed 1\n",
      "\n",
      "[Seed 1] Training TFT (input=90, output=30, INTERNAL val for early-stopping, OFFICIAL val/test for reporting) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                              | Type                             | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0      | train\n",
      "1  | val_metrics                       | MetricCollection                 | 0      | train\n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0      | train\n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 138 K  | train\n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 5.2 K  | train\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 1.6 K  | train\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K | train\n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K | train\n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K | train\n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K | train\n",
      "10 | lstm_encoder                      | LSTM                             | 33.3 K | train\n",
      "11 | lstm_decoder                      | LSTM                             | 33.3 K | train\n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K  | train\n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K | train\n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K | train\n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K  | train\n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K | train\n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K  | train\n",
      "18 | output_layer                      | Linear                           | 1.1 K  | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "352 K     Trainable params\n",
      "0         Non-trainable params\n",
      "352 K     Total params\n",
      "1.412     Total estimated model params size (MB)\n",
      "1101      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fe58b66eef4651ab3ca449c9830712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe76726bf35f4bdfb425a315b88a935c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0879daa3fba04c7bbf1811c933264aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45245409f6d549459e2861c8ceb08a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef9175f5b7d4dfc8aee5d3cc6d18daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8853cb9385f4762999aa95422c3e977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9f1c9c743a4c8589d044ce28113c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403b474efd484aafa39814f26535b26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e810b0e21f4fb18246016121ef8b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a05733268cf428cb366606e94093a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34f8460d5084aa3b029e2ca0d29353c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb3b489caa04061993180c897fa632d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebd3d3b7f6240c9bdb5983ff7d0e649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5962d6bb810f4dd590d85cb2c37fb2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d584aeeb368a41b28d1880e1172661f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ece7b2d28124607be34cb089ea6e930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4caa465c50241f9b157f30d9d371dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eb902294924335b6b105d2ede1b774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7defb57fc842bd985ec6632f812338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d6c6d5d6764ca9be532905a9d8e580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a34add8a634ec3a0e236c2e3609ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5bd686d98f4f04933d80c112073985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded (best by INTERNAL val_loss).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TFT_STORE_H30 OFFICIAL eval (paper window; report only) ===\n",
      "Val  RMSE=0.215322, MAE=0.166161, R2=-2.109778\n",
      "Test RMSE=0.186304, MAE=0.145831, R2=-1.301501\n",
      "\n",
      "=== P50 q-risk (Val) ===\n",
      "qrisk_p50_scaled_log = 0.427480\n",
      "qrisk_p50_unscaled_log = 0.036526\n",
      "qrisk_p50_sales = 0.311621\n",
      "\n",
      "=== P50 q-risk (Test) ===\n",
      "qrisk_p50_scaled_log = 0.434851\n",
      "qrisk_p50_unscaled_log = 0.032575\n",
      "qrisk_p50_sales = 0.321846\n",
      "Using metrics.csv: /home/wangxc1117/TFTModel-use/geospatial-neural-adapter-dev/examples/try/use_admm_crood/sales_forecasting/TFT/sales_TFT_ADMM/store_level_new/compare_cov/no_future_cov/TFT_runs_favorita_store_paperWindow_h30_clean_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd/TFT_Favorita_seed_1/tft_favorita_store_paperWindow_h30_seed1_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd/version_0/metrics.csv\n",
      "[plot_tft_loss_curve] Saved -> /home/wangxc1117/TFTModel-use/geospatial-neural-adapter-dev/examples/try/use_admm_crood/sales_forecasting/TFT/sales_TFT_ADMM/store_level_new/compare_cov/no_future_cov/tft_loss_favorita_store_h30_seed1_BATCH_oilStd_noFutureCov_addRelIdx.png\n",
      "\n",
      "Start plotting ALL 53 stores (2 figs/store) ...\n",
      "All figures saved under: /home/wangxc1117/TFTModel-use/geospatial-neural-adapter-dev/examples/try/use_admm_crood/sales_forecasting/TFT/sales_TFT_ADMM/store_level_new/compare_cov/no_future_cov/TFT_plots_favorita_store_paperWindow_h30_clean_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd\n",
      "Completed seed 1\n",
      "\n",
      "All TFT STORE-level H30 experiments completed!\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    clear_gpu_memory,\n",
    "    create_experiment_config,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling\n",
    "\n",
    "\n",
    "# Global settings & dirs\n",
    "MODE = \"train\"\n",
    "\n",
    "try:\n",
    "    EXP_ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    EXP_ROOT = Path.cwd()\n",
    "\n",
    "CKPT_DIR = (EXP_ROOT / \"darts_ckpt_favorita_store_paperWindow_h30_clean_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd\").resolve()\n",
    "RUNS_DIR = (EXP_ROOT / \"TFT_runs_favorita_store_paperWindow_h30_clean_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd\").resolve()\n",
    "PLOTS_DIR = (EXP_ROOT / \"TFT_plots_favorita_store_paperWindow_h30_clean_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd\").resolve()\n",
    "\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info[\"device\"] == \"cuda\":\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "\n",
    "# TFT config\n",
    "TREND_CONFIG: Dict[str, Any] = {\n",
    "    \"input_chunk_length\": 90,\n",
    "    \"output_chunk_length\": 30,\n",
    "    \"n_epochs\": 20,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_size\": 32,\n",
    "    \"optimizer_kwargs\": {\"lr\": 3e-4},\n",
    "    \"random_state\": GLOBAL_SEED,\n",
    "    \"force_reset\": True,\n",
    "    \"full_attention\": False,\n",
    "    \"add_relative_index\": True,\n",
    "    \"pl_trainer_kwargs\": {\n",
    "        \"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"devices\": 1,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"enable_model_summary\": False,\n",
    "        \"enable_checkpointing\": True,\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "        \"callbacks\": [\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "                patience=5,\n",
    "                min_delta=5e-4,\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "L = int(TREND_CONFIG[\"input_chunk_length\"])\n",
    "H = int(TREND_CONFIG[\"output_chunk_length\"])\n",
    "TAG = f\"H{H}\"\n",
    "TAG_LOWER = f\"h{H}\"\n",
    "\n",
    "print(f\"\\n=== TFT (STORE-level): input={L}, output={H} (30-step direct predict) ===\")\n",
    "print(\"Config: add_relative_index=True (only time feature).\")\n",
    "print(\"Future covariates: NONE\")\n",
    "print(\"Past covariates  : oil_std ONLY\")\n",
    "print(\"Static covariates: one-hot store meta\")\n",
    "print(\"OFFICIAL inference: BATCH predict\")\n",
    "print(\"oil standardized using TRAIN-only mean/std\")\n",
    "\n",
    "\n",
    "# Paper window & splits (reporting only)\n",
    "PAPER_TRAIN_START = pd.Timestamp(\"2015-01-01\")\n",
    "PAPER_TRAIN_END = pd.Timestamp(\"2015-12-01\")\n",
    "\n",
    "PAPER_VAL_DAYS = 30\n",
    "PAPER_TEST_DAYS = 30\n",
    "\n",
    "PAPER_VAL_START = PAPER_TRAIN_END + pd.Timedelta(days=1)\n",
    "PAPER_VAL_END = PAPER_VAL_START + pd.Timedelta(days=PAPER_VAL_DAYS - 1)\n",
    "\n",
    "PAPER_TEST_START = PAPER_VAL_END + pd.Timedelta(days=1)\n",
    "PAPER_TEST_END = PAPER_TEST_START + pd.Timedelta(days=PAPER_TEST_DAYS - 1)\n",
    "\n",
    "PAPER_ALL_START = PAPER_TRAIN_START\n",
    "PAPER_ALL_END = PAPER_TEST_END\n",
    "\n",
    "print(\"\\n=== OFFICIAL (paper-like) window (for reporting/eval only) ===\")\n",
    "print(\"Train:\", PAPER_TRAIN_START.date(), \"→\", PAPER_TRAIN_END.date())\n",
    "print(\"Val  :\", PAPER_VAL_START.date(), \"→\", PAPER_VAL_END.date())\n",
    "print(\"Test :\", PAPER_TEST_START.date(), \"→\", PAPER_TEST_END.date())\n",
    "print(\"All  :\", PAPER_ALL_START.date(), \"→\", PAPER_ALL_END.date())\n",
    "\n",
    "\n",
    "# Load raw files\n",
    "DATA_ROOT = Path(\"/home/wangxc1117/experiment_data/sales_forecasting_data\")\n",
    "TRAIN_PATH = DATA_ROOT / \"train.csv\"\n",
    "STORES_PATH = DATA_ROOT / \"stores.csv\"\n",
    "OIL_PATH = DATA_ROOT / \"oil.csv\"\n",
    "TRANSACTIONS_PATH = DATA_ROOT / \"transactions.csv\"\n",
    "\n",
    "for p, name in [\n",
    "    (TRAIN_PATH, \"train.csv\"),\n",
    "    (STORES_PATH, \"stores.csv\"),\n",
    "    (OIL_PATH, \"oil.csv\"),\n",
    "]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{name} not found at {p}\")\n",
    "\n",
    "print(\"\\n=== Loading train.csv ===\")\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"onpromotion\"] = df[\"onpromotion\"].fillna(0).astype(int)\n",
    "df = df[(df[\"date\"] >= PAPER_ALL_START) & (df[\"date\"] <= PAPER_ALL_END)].copy()\n",
    "print(f\"Paper-window date range (in df): {df['date'].min()} → {df['date'].max()}\")\n",
    "\n",
    "\n",
    "# Build store-level target\n",
    "df_store_sales = (\n",
    "    df.groupby([\"date\", \"store_nbr\"], as_index=False)[\"unit_sales\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"unit_sales\": \"store_sales\"})\n",
    ")\n",
    "df_store_sales[\"store_sales\"] = df_store_sales[\"store_sales\"].clip(lower=0.0)\n",
    "\n",
    "stores = sorted(df_store_sales[\"store_nbr\"].unique())\n",
    "date_index = pd.date_range(start=PAPER_ALL_START, end=PAPER_ALL_END, freq=\"D\")\n",
    "T_all = len(date_index)\n",
    "N = len(stores)\n",
    "\n",
    "print(f\"\\nUsing ALL {N} stores in paper window.\")\n",
    "print(f\"T_all (days): {T_all}\")\n",
    "\n",
    "full_idx = pd.MultiIndex.from_product([date_index, stores], names=[\"date\", \"store_nbr\"])\n",
    "\n",
    "panel_sales_raw = (\n",
    "    df_store_sales.set_index([\"date\", \"store_nbr\"])\n",
    "    .reindex(full_idx)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "panel_sales = panel_sales_raw.copy()\n",
    "panel_sales[\"store_sales\"] = panel_sales.groupby(\"store_nbr\")[\"store_sales\"].ffill()\n",
    "panel_sales.loc[panel_sales[\"store_sales\"].isna(), \"store_sales\"] = 0.0\n",
    "panel_sales[\"log_sales\"] = np.log1p(panel_sales[\"store_sales\"]).astype(\"float32\")\n",
    "\n",
    "Y_df = (\n",
    "    panel_sales[\"log_sales\"]\n",
    "    .unstack(\"store_nbr\")\n",
    "    .reindex(index=date_index, columns=stores)\n",
    "    .astype(\"float32\")\n",
    ")\n",
    "targets_full = Y_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "print(\"\\n=== Target sanity ===\")\n",
    "print(\"targets_full shape:\", targets_full.shape)\n",
    "print(\"targets_full has_na:\", bool(np.isnan(targets_full).any()))\n",
    "\n",
    "\n",
    "# Static cov + oil + transactions (loaded only)\n",
    "stores_meta = pd.read_csv(STORES_PATH).set_index(\"store_nbr\").loc[stores]\n",
    "\n",
    "static_cat = stores_meta[[\"city\", \"state\", \"type\", \"cluster\"]].copy()\n",
    "static_oh = pd.get_dummies(static_cat.astype(str), drop_first=False)\n",
    "static_cov_df = static_oh.astype(\"float32\")\n",
    "print(\"\\nStatic cov (one-hot) shape:\", static_cov_df.shape)\n",
    "\n",
    "oil = pd.read_csv(OIL_PATH)\n",
    "oil[\"date\"] = pd.to_datetime(oil[\"date\"])\n",
    "oil = oil.set_index(\"date\")[\"dcoilwtico\"].reindex(date_index).astype(\"float32\")\n",
    "oil = oil.ffill().bfill()\n",
    "oil_vals = oil.to_numpy().reshape(-1, 1)\n",
    "oil_mat = np.repeat(oil_vals, N, axis=1).astype(\"float32\")\n",
    "print(\"Oil covariate: has_na =\", bool(np.isnan(oil_mat).any()))\n",
    "\n",
    "if TRANSACTIONS_PATH.exists():\n",
    "    tr = pd.read_csv(TRANSACTIONS_PATH)\n",
    "    tr[\"date\"] = pd.to_datetime(tr[\"date\"])\n",
    "    tr = tr[(tr[\"date\"] >= PAPER_ALL_START) & (tr[\"date\"] <= PAPER_ALL_END)].copy()\n",
    "\n",
    "    tr_panel = (\n",
    "        tr.set_index([\"date\", \"store_nbr\"])[\"transactions\"]\n",
    "        .reindex(full_idx)\n",
    "        .sort_index()\n",
    "        .astype(\"float32\")\n",
    "    )\n",
    "    tr_panel = tr_panel.fillna(0.0)\n",
    "    tr_panel = tr_panel.groupby(\"store_nbr\").ffill().fillna(0.0)\n",
    "    tr_mat = (\n",
    "        tr_panel.unstack(\"store_nbr\")\n",
    "        .reindex(index=date_index, columns=stores)\n",
    "        .astype(\"float32\")\n",
    "        .to_numpy()\n",
    "    )\n",
    "    print(\"Transactions loaded (NOT used): has_na =\", bool(np.isnan(tr_mat).any()))\n",
    "else:\n",
    "    tr_mat = np.zeros((T_all, N), dtype=np.float32)\n",
    "    print(\"Transactions not found (NOT used): using zeros placeholder.\")\n",
    "\n",
    "\n",
    "# Official cuts (reporting only)\n",
    "cut_train = int((PAPER_TRAIN_END - PAPER_ALL_START).days + 1)\n",
    "cut_val = cut_train + PAPER_VAL_DAYS\n",
    "assert cut_val + PAPER_TEST_DAYS == T_all\n",
    "\n",
    "print(\"\\n=== OFFICIAL split lengths (reporting only) ===\")\n",
    "print(\"T_all:\", T_all, \"| train:\", cut_train, \"| val:\", (cut_val - cut_train), \"| test:\", (T_all - cut_val))\n",
    "\n",
    "\n",
    "# Scaling targets only (fit on train only)\n",
    "cat_dummy = np.zeros((T_all, N, 1), dtype=np.int64)\n",
    "cont_dummy = np.zeros((T_all, N, 1), dtype=np.float32)\n",
    "\n",
    "train_ratio = cut_train / T_all\n",
    "val_ratio = PAPER_VAL_DAYS / T_all\n",
    "\n",
    "train_dataset_scl, val_dataset_scl, test_dataset_scl, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_dummy,\n",
    "    cont_features=cont_dummy,\n",
    "    targets=targets_full,\n",
    "    train_ratio=train_ratio,\n",
    "    val_ratio=val_ratio,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def stitch_y(dsets) -> np.ndarray:\n",
    "    ys = [ds.tensors[2].cpu().numpy().astype(np.float32) for ds in dsets]\n",
    "    return np.concatenate(ys, axis=0)\n",
    "\n",
    "\n",
    "y_all_s = stitch_y([train_dataset_scl, val_dataset_scl, test_dataset_scl])\n",
    "assert y_all_s.shape == targets_full.shape\n",
    "print(\"y_all_s shape:\", y_all_s.shape)\n",
    "\n",
    "\n",
    "# Standardize oil (train only)\n",
    "EPS_STD = 1e-6\n",
    "\n",
    "\n",
    "def train_only_standardize_2d(x_2d: np.ndarray, cut: int) -> Tuple[np.ndarray, float, float]:\n",
    "    x_train = x_2d[:cut, :].astype(np.float64)\n",
    "    mean = float(np.mean(x_train))\n",
    "    std = float(np.std(x_train, ddof=0))\n",
    "    if std < EPS_STD:\n",
    "        std = 1.0\n",
    "    x_std = ((x_2d.astype(np.float64) - mean) / std).astype(np.float32)\n",
    "    return x_std, mean, std\n",
    "\n",
    "\n",
    "oil_mat_s, oil_mean, oil_std = train_only_standardize_2d(oil_mat, cut_train)\n",
    "print(\"\\n=== TRAIN-only standardization stats ===\")\n",
    "print(f\"oil: mean={oil_mean:.6f}, std={oil_std:.6f}\")\n",
    "\n",
    "\n",
    "# q-risk helpers\n",
    "def quantile_loss(y_true: np.ndarray, y_pred: np.ndarray, q: float) -> np.ndarray:\n",
    "    e = y_true - y_pred\n",
    "    return np.maximum(q * e, (q - 1.0) * e)\n",
    "\n",
    "\n",
    "def qrisk(y_true: np.ndarray, y_pred: np.ndarray, q: float, eps: float = 1e-8) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    num = 2.0 * np.sum(quantile_loss(y_true, y_pred, q))\n",
    "    den = np.sum(np.abs(y_true)) + eps\n",
    "    return float(num / den)\n",
    "\n",
    "\n",
    "def _try_get_target_scaler_params(prep) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    if prep is None:\n",
    "        return None, None\n",
    "    ts = getattr(prep, \"target_scaler\", None)\n",
    "    if ts is None:\n",
    "        return None, None\n",
    "    mean_ = getattr(ts, \"mean_\", None)\n",
    "    scale_ = getattr(ts, \"scale_\", None)\n",
    "    if mean_ is None or scale_ is None:\n",
    "        return None, None\n",
    "    return np.asarray(mean_, dtype=np.float64), np.asarray(scale_, dtype=np.float64)\n",
    "\n",
    "\n",
    "TARGET_MEAN, TARGET_SCALE = _try_get_target_scaler_params(preprocessor)\n",
    "\n",
    "\n",
    "def inverse_scale_log(y_scaled: np.ndarray) -> Optional[np.ndarray]:\n",
    "    if TARGET_MEAN is None or TARGET_SCALE is None:\n",
    "        return None\n",
    "    y_scaled = np.asarray(y_scaled, dtype=np.float64)\n",
    "\n",
    "    m = TARGET_MEAN\n",
    "    s = TARGET_SCALE\n",
    "\n",
    "    if m.ndim == 1 and m.shape[0] == y_scaled.shape[1]:\n",
    "        m2 = m.reshape(1, -1)\n",
    "    elif m.size == 1:\n",
    "        m2 = np.full((1, y_scaled.shape[1]), float(m.reshape(-1)[0]), dtype=np.float64)\n",
    "    else:\n",
    "        m2 = m.reshape(1, -1) if m.ndim == 1 else m\n",
    "\n",
    "    if s.ndim == 1 and s.shape[0] == y_scaled.shape[1]:\n",
    "        s2 = s.reshape(1, -1)\n",
    "    elif s.size == 1:\n",
    "        s2 = np.full((1, y_scaled.shape[1]), float(s.reshape(-1)[0]), dtype=np.float64)\n",
    "    else:\n",
    "        s2 = s.reshape(1, -1) if s.ndim == 1 else s\n",
    "\n",
    "    return (y_scaled * s2 + m2).astype(np.float32)\n",
    "\n",
    "\n",
    "def log_to_sales(y_log: np.ndarray) -> np.ndarray:\n",
    "    y = np.expm1(np.asarray(y_log, dtype=np.float64))\n",
    "    y = np.clip(y, 0.0, None)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_p50_qrisk_block(y_true_2d: np.ndarray, y_pred_2d: np.ndarray) -> Dict[str, float]:\n",
    "    out: Dict[str, float] = {}\n",
    "    out[\"qrisk_p50_scaled_log\"] = qrisk(y_true_2d, y_pred_2d, q=0.5)\n",
    "\n",
    "    y_true_log = inverse_scale_log(y_true_2d)\n",
    "    y_pred_log = inverse_scale_log(y_pred_2d)\n",
    "    if y_true_log is not None and y_pred_log is not None:\n",
    "        out[\"qrisk_p50_unscaled_log\"] = qrisk(y_true_log, y_pred_log, q=0.5)\n",
    "        out[\"qrisk_p50_sales\"] = qrisk(log_to_sales(y_true_log), log_to_sales(y_pred_log), q=0.5)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Build TimeSeries lists (target + static + past)\n",
    "series_all: List[TimeSeries] = []\n",
    "pcov_all_series: List[TimeSeries] = []\n",
    "\n",
    "for j, sid in enumerate(stores):\n",
    "    name = f\"store_{sid}\"\n",
    "\n",
    "    ts = TimeSeries.from_times_and_values(\n",
    "        times=date_index,\n",
    "        values=y_all_s[:, j:j + 1],\n",
    "        columns=[name],\n",
    "        freq=\"D\",\n",
    "    )\n",
    "\n",
    "    sc = static_cov_df.loc[[sid]].copy()\n",
    "    sc.index = [name]\n",
    "    ts = ts.with_static_covariates(sc)\n",
    "    series_all.append(ts)\n",
    "\n",
    "    pc_vals = oil_mat_s[:, j:j + 1].astype(\"float32\")\n",
    "    pcov = TimeSeries.from_times_and_values(\n",
    "        times=date_index,\n",
    "        values=pc_vals,\n",
    "        columns=[f\"{name}_oil_std\"],\n",
    "        freq=\"D\",\n",
    "    )\n",
    "    pcov_all_series.append(pcov)\n",
    "\n",
    "print(f\"\\nBuilt {len(series_all)} TimeSeries. Future covariates: NONE. Past cov: oil_std only.\")\n",
    "\n",
    "\n",
    "# Internal validation slice\n",
    "INTERNAL_VAL_DAYS = 240\n",
    "\n",
    "\n",
    "def slice_list(ts_list: List[TimeSeries], a: int, b: int) -> List[TimeSeries]:\n",
    "    return [ts[a:b] for ts in ts_list]\n",
    "\n",
    "\n",
    "train_series_in = slice_list(series_all, 0, cut_train)\n",
    "train_pcov_in = slice_list(pcov_all_series, 0, cut_train)\n",
    "\n",
    "internal_val_start = max(0, cut_train - INTERNAL_VAL_DAYS)\n",
    "internal_val_end = cut_train\n",
    "min_needed = L + H\n",
    "if (internal_val_end - internal_val_start) < min_needed:\n",
    "    internal_val_start = max(0, internal_val_end - min_needed)\n",
    "\n",
    "val_series_in = slice_list(series_all, internal_val_start, internal_val_end)\n",
    "val_pcov_in = slice_list(pcov_all_series, internal_val_start, internal_val_end)\n",
    "\n",
    "print(\"\\n=== INTERNAL validation (for early stopping only) ===\")\n",
    "print(\"Train idx: [0 :\", cut_train, \") len =\", len(train_series_in[0]))\n",
    "print(\"IntVal idx: [\", internal_val_start, \":\", internal_val_end, \") len =\", len(val_series_in[0]))\n",
    "print(\"IntVal covers dates:\", date_index[internal_val_start].date(), \"→\", date_index[internal_val_end - 1].date())\n",
    "\n",
    "\n",
    "# Official inference (batch predict)\n",
    "def infer_official_direct_30_batch(tft: TFTModel) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    val_expected_index = date_index[cut_train:cut_val]\n",
    "    test_expected_index = date_index[cut_val:]\n",
    "\n",
    "    ts_ctx_val_list = slice_list(series_all, 0, cut_train)\n",
    "    pc_ctx_val_list = slice_list(pcov_all_series, 0, cut_train)\n",
    "\n",
    "    pred_val_list = tft.predict(\n",
    "        n=H,\n",
    "        series=ts_ctx_val_list,\n",
    "        past_covariates=pc_ctx_val_list,\n",
    "        verbose=False,\n",
    "    )\n",
    "    if isinstance(pred_val_list, TimeSeries):\n",
    "        pred_val_list = [pred_val_list]\n",
    "    if len(pred_val_list) != N:\n",
    "        raise RuntimeError(f\"[Val] expected {N} series predictions, got {len(pred_val_list)}\")\n",
    "\n",
    "    yhat_val = np.full((PAPER_VAL_DAYS, N), np.nan, dtype=np.float32)\n",
    "    for j, pred in enumerate(pred_val_list):\n",
    "        if len(pred) != PAPER_VAL_DAYS:\n",
    "            raise RuntimeError(f\"[Val] store idx {j}: expected 30 preds, got {len(pred)}\")\n",
    "        if not pred.time_index.equals(val_expected_index):\n",
    "            raise RuntimeError(\"[Val] time_index mismatch\")\n",
    "        yhat_val[:, j] = pred.values(copy=False).astype(np.float32)[:, 0]\n",
    "\n",
    "    ts_ctx_test_list = slice_list(series_all, 0, cut_val)\n",
    "    pc_ctx_test_list = slice_list(pcov_all_series, 0, cut_val)\n",
    "\n",
    "    pred_test_list = tft.predict(\n",
    "        n=H,\n",
    "        series=ts_ctx_test_list,\n",
    "        past_covariates=pc_ctx_test_list,\n",
    "        verbose=False,\n",
    "    )\n",
    "    if isinstance(pred_test_list, TimeSeries):\n",
    "        pred_test_list = [pred_test_list]\n",
    "    if len(pred_test_list) != N:\n",
    "        raise RuntimeError(f\"[Test] expected {N} series predictions, got {len(pred_test_list)}\")\n",
    "\n",
    "    yhat_test = np.full((PAPER_TEST_DAYS, N), np.nan, dtype=np.float32)\n",
    "    for j, pred in enumerate(pred_test_list):\n",
    "        if len(pred) != PAPER_TEST_DAYS:\n",
    "            raise RuntimeError(f\"[Test] store idx {j}: expected 30 preds, got {len(pred)}\")\n",
    "        if not pred.time_index.equals(test_expected_index):\n",
    "            raise RuntimeError(\"[Test] time_index mismatch\")\n",
    "        yhat_test[:, j] = pred.values(copy=False).astype(np.float32)[:, 0]\n",
    "\n",
    "    if np.isnan(yhat_val).any() or np.isnan(yhat_test).any():\n",
    "        raise RuntimeError(\"Inference produced NaNs.\")\n",
    "\n",
    "    return yhat_val, yhat_test\n",
    "\n",
    "\n",
    "# Loss curve plot\n",
    "def find_metrics_csv(base_dir: Path, dataset_seed: int, model_name: str) -> Path:\n",
    "    base = Path(base_dir) / f\"TFT_Favorita_seed_{dataset_seed}\" / model_name\n",
    "    pattern = str(base / \"version_*\" / \"metrics.csv\")\n",
    "    candidates = glob.glob(pattern)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"metrics.csv not found; pattern: {pattern}\")\n",
    "    latest = max(candidates, key=lambda p: Path(p).stat().st_mtime)\n",
    "    return Path(latest).resolve()\n",
    "\n",
    "\n",
    "def plot_tft_loss_curve(metrics_csv_path: Path, out_path: Path):\n",
    "    if not metrics_csv_path.exists():\n",
    "        print(f\"[plot_tft_loss_curve] {metrics_csv_path} not found, skip.\")\n",
    "        return\n",
    "\n",
    "    dfm = pd.read_csv(metrics_csv_path)\n",
    "    dfm.columns = [c.replace(\"/\", \"_\").replace(\".\", \"_\") for c in dfm.columns]\n",
    "\n",
    "    if \"epoch\" not in dfm.columns:\n",
    "        print(\"[plot_tft_loss_curve] no epoch col, skip.\")\n",
    "        return\n",
    "\n",
    "    loss_cols = [c for c in dfm.columns if \"loss\" in c.lower() and pd.api.types.is_numeric_dtype(dfm[c])]\n",
    "    if not loss_cols:\n",
    "        print(\"[plot_tft_loss_curve] no numeric loss col, skip.\")\n",
    "        return\n",
    "\n",
    "    grp = dfm[[\"epoch\"] + loss_cols].groupby(\"epoch\", as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    train_col = \"train_loss\" if \"train_loss\" in grp.columns else (\"loss\" if \"loss\" in grp.columns else None)\n",
    "    val_col = next((c for c in grp.columns if \"loss\" in c.lower() and \"val\" in c.lower()), None)\n",
    "\n",
    "    plt.figure(figsize=(8, 4), dpi=140)\n",
    "    if train_col is not None:\n",
    "        plt.plot(grp[\"epoch\"], grp[train_col], \"-\", linewidth=2, label=\"train_loss (mean/epoch)\")\n",
    "    if val_col is not None:\n",
    "        plt.plot(grp[\"epoch\"], grp[val_col], \"-o\", linewidth=2, markersize=4, label=\"val_loss (INTERNAL slice)\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"TFT loss vs epoch (internal val for early stopping)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "    print(f\"[plot_tft_loss_curve] Saved -> {out_path}\")\n",
    "\n",
    "\n",
    "# Plotting\n",
    "def plot_store_two_figs(\n",
    "    sid: int,\n",
    "    y_true_scaled_full: np.ndarray,\n",
    "    yhat_test_scaled: np.ndarray,\n",
    "    out_dir: Path,\n",
    "    dataset_seed: int,\n",
    "):\n",
    "    j = stores.index(sid)\n",
    "\n",
    "    y_val_true = y_true_scaled_full[cut_train:cut_val, j]\n",
    "    y_test_true = y_true_scaled_full[cut_val:, j]\n",
    "    y_test_pred = yhat_test_scaled[:, j]\n",
    "\n",
    "    dates_val = date_index[cut_train:cut_val]\n",
    "    dates_test = date_index[cut_val:]\n",
    "\n",
    "    dates_true_vt = dates_val.append(dates_test)\n",
    "    y_true_vt = np.concatenate([y_val_true, y_test_true], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=140)\n",
    "    plt.plot(dates_true_vt, y_true_vt, \"-\", linewidth=2.0, color=\"k\", label=\"True (Val+Test)\")\n",
    "    plt.plot(dates_test, y_test_pred, \"-\", linewidth=1.8, color=\"C3\", label=\"Test Pred\")\n",
    "    plt.axvline(date_index[cut_train], linestyle=\"--\", linewidth=1, label=\"train/val split\")\n",
    "    plt.axvline(date_index[cut_val], linestyle=\"--\", linewidth=1, label=\"val/test split\")\n",
    "    plt.title(f\"FIG1 {TAG} store {sid}: True (continuous) + Test Pred\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Scaled log_sales\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    p1 = out_dir / f\"FIG1_store{sid}_{TAG_LOWER}_seed{dataset_seed}.png\"\n",
    "    plt.savefig(p1)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=140)\n",
    "    y_all_true = y_true_scaled_full[:, j]\n",
    "    plt.plot(date_index, y_all_true, \"-\", linewidth=1.8, color=\"k\", label=\"All True\")\n",
    "    plt.plot(dates_test, y_test_pred, \"-\", linewidth=1.8, color=\"C3\", label=\"Test Pred\")\n",
    "    plt.axvline(date_index[cut_train], linestyle=\"--\", linewidth=1, label=\"train/val split\")\n",
    "    plt.axvline(date_index[cut_val], linestyle=\"--\", linewidth=1, label=\"val/test split\")\n",
    "    plt.title(f\"FIG2 {TAG} store {sid}: All True + Test Pred\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Scaled log_sales\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    p2 = out_dir / f\"FIG2_store{sid}_{TAG_LOWER}_seed{dataset_seed}.png\"\n",
    "    plt.savefig(p2)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_all_stores_two_figs(\n",
    "    y_true_scaled_full: np.ndarray,\n",
    "    yhat_test_scaled: np.ndarray,\n",
    "    dataset_seed: int,\n",
    "):\n",
    "    print(f\"\\nStart plotting ALL {len(stores)} stores (2 figs/store) ...\")\n",
    "    for sid in stores:\n",
    "        plot_store_two_figs(\n",
    "            sid=sid,\n",
    "            y_true_scaled_full=y_true_scaled_full,\n",
    "            yhat_test_scaled=yhat_test_scaled,\n",
    "            out_dir=PLOTS_DIR,\n",
    "            dataset_seed=dataset_seed,\n",
    "        )\n",
    "    print(f\"All figures saved under: {PLOTS_DIR}\")\n",
    "\n",
    "\n",
    "# Runner\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=1,\n",
    "    n_dataset_seeds=1,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=2,\n",
    ")\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "\n",
    "\n",
    "def build_model(model_name: str, log_root: Path) -> TFTModel:\n",
    "    pl_kwargs = dict(TREND_CONFIG[\"pl_trainer_kwargs\"])\n",
    "    pl_kwargs[\"logger\"] = CSVLogger(save_dir=str(log_root), name=model_name)\n",
    "\n",
    "    return TFTModel(\n",
    "        input_chunk_length=L,\n",
    "        output_chunk_length=H,\n",
    "        n_epochs=TREND_CONFIG[\"n_epochs\"],\n",
    "        hidden_size=TREND_CONFIG[\"hidden_size\"],\n",
    "        num_attention_heads=TREND_CONFIG[\"num_attention_heads\"],\n",
    "        dropout=TREND_CONFIG[\"dropout\"],\n",
    "        batch_size=TREND_CONFIG[\"batch_size\"],\n",
    "        random_state=TREND_CONFIG[\"random_state\"],\n",
    "        force_reset=TREND_CONFIG[\"force_reset\"],\n",
    "        full_attention=TREND_CONFIG[\"full_attention\"],\n",
    "        add_relative_index=TREND_CONFIG[\"add_relative_index\"],\n",
    "        pl_trainer_kwargs=pl_kwargs,\n",
    "        model_name=model_name,\n",
    "        work_dir=str(CKPT_DIR),\n",
    "        save_checkpoints=True,\n",
    "        optimizer_kwargs=TREND_CONFIG.get(\"optimizer_kwargs\", None),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_one_seed(dataset_seed: int) -> str:\n",
    "    np.random.seed(dataset_seed)\n",
    "    torch.manual_seed(dataset_seed)\n",
    "\n",
    "    log_root = RUNS_DIR / f\"TFT_Favorita_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_name = f\"tft_favorita_store_paperWindow_{TAG_LOWER}_seed{dataset_seed}_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd\"\n",
    "\n",
    "    print(\n",
    "        f\"\\n[Seed {dataset_seed}] Training TFT \"\n",
    "        f\"(input={L}, output={H}, INTERNAL val for early-stopping, OFFICIAL val/test for reporting) ...\"\n",
    "    )\n",
    "\n",
    "    tft = build_model(model_name=model_name, log_root=log_root)\n",
    "\n",
    "    tft.fit(\n",
    "        series=train_series_in,\n",
    "        past_covariates=train_pcov_in,\n",
    "        val_series=val_series_in,\n",
    "        val_past_covariates=val_pcov_in,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return model_name\n",
    "\n",
    "\n",
    "def load_best_ckpt(model_name: str) -> TFTModel:\n",
    "    tft = TFTModel.load_from_checkpoint(model_name=model_name, work_dir=str(CKPT_DIR), best=True)\n",
    "    print(\"Checkpoint loaded (best by INTERNAL val_loss).\")\n",
    "    return tft\n",
    "\n",
    "\n",
    "def official_eval_and_save(dataset_seed: int, model_name: str) -> None:\n",
    "    tft = load_best_ckpt(model_name=model_name)\n",
    "\n",
    "    yhat_val_scaled, yhat_test_scaled = infer_official_direct_30_batch(tft)\n",
    "\n",
    "    y_val_true = y_all_s[cut_train:cut_val, :]\n",
    "    y_test_true = y_all_s[cut_val:, :]\n",
    "\n",
    "    val_y_t = torch.from_numpy(y_val_true).to(DEVICE)\n",
    "    test_y_t = torch.from_numpy(y_test_true).to(DEVICE)\n",
    "    yhat_val_t = torch.from_numpy(yhat_val_scaled).to(DEVICE)\n",
    "    yhat_test_t = torch.from_numpy(yhat_test_scaled).to(DEVICE)\n",
    "\n",
    "    rmse_v, mae_v, r2_v = compute_metrics(val_y_t, yhat_val_t)\n",
    "    rmse_t, mae_t, r2_t = compute_metrics(test_y_t, yhat_test_t)\n",
    "\n",
    "    qrisk_val = compute_p50_qrisk_block(y_val_true, yhat_val_scaled)\n",
    "    qrisk_test = compute_p50_qrisk_block(y_test_true, yhat_test_scaled)\n",
    "\n",
    "    print(f\"\\n=== TFT_STORE_{TAG} OFFICIAL eval (paper window; report only) ===\")\n",
    "    print(f\"Val  RMSE={rmse_v:.6f}, MAE={mae_v:.6f}, R2={r2_v:.6f}\")\n",
    "    print(f\"Test RMSE={rmse_t:.6f}, MAE={mae_t:.6f}, R2={r2_t:.6f}\")\n",
    "\n",
    "    print(\"\\n=== P50 q-risk (Val) ===\")\n",
    "    for k, v in qrisk_val.items():\n",
    "        print(f\"{k} = {v:.6f}\")\n",
    "\n",
    "    print(\"\\n=== P50 q-risk (Test) ===\")\n",
    "    for k, v in qrisk_test.items():\n",
    "        print(f\"{k} = {v:.6f}\")\n",
    "\n",
    "    csv_path = EXP_ROOT / f\"metrics_summary_TFT_Favorita_STORE_{TAG}_paperWindow_clean_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd.csv\"\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\n",
    "                \"seed\", \"model\",\n",
    "                \"rmse_val\", \"mae_val\", \"r2_val\",\n",
    "                \"rmse_test\", \"mae_test\", \"r2_test\",\n",
    "                \"val_qrisk_p50_scaled_log\", \"val_qrisk_p50_unscaled_log\", \"val_qrisk_p50_sales\",\n",
    "                \"test_qrisk_p50_scaled_log\", \"test_qrisk_p50_unscaled_log\", \"test_qrisk_p50_sales\",\n",
    "                \"oil_mean_train\", \"oil_std_train\",\n",
    "            ])\n",
    "\n",
    "        model_label = f\"TFT_STORE_{TAG}_paperWindow_clean_noFutureCov_oilStd_staticOH_addRelIdx_noEnc_BATCH\"\n",
    "        w.writerow([\n",
    "            dataset_seed, model_label,\n",
    "            float(rmse_v), float(mae_v), float(r2_v),\n",
    "            float(rmse_t), float(mae_t), float(r2_t),\n",
    "            float(qrisk_val[\"qrisk_p50_scaled_log\"]),\n",
    "            float(qrisk_val.get(\"qrisk_p50_unscaled_log\", np.nan)),\n",
    "            float(qrisk_val.get(\"qrisk_p50_sales\", np.nan)),\n",
    "            float(qrisk_test[\"qrisk_p50_scaled_log\"]),\n",
    "            float(qrisk_test.get(\"qrisk_p50_unscaled_log\", np.nan)),\n",
    "            float(qrisk_test.get(\"qrisk_p50_sales\", np.nan)),\n",
    "            float(oil_mean), float(oil_std),\n",
    "        ])\n",
    "\n",
    "    try:\n",
    "        metrics_csv_path = find_metrics_csv(RUNS_DIR, dataset_seed=dataset_seed, model_name=model_name)\n",
    "        loss_png = EXP_ROOT / f\"tft_loss_favorita_store_{TAG_LOWER}_seed{dataset_seed}_BATCH_oilStd_noFutureCov_addRelIdx.png\"\n",
    "        print(\"Using metrics.csv:\", metrics_csv_path)\n",
    "        plot_tft_loss_curve(metrics_csv_path, loss_png)\n",
    "    except Exception as e:\n",
    "        print(\"[Loss plot] skipped due to:\", e)\n",
    "\n",
    "    plot_all_stores_two_figs(\n",
    "        y_true_scaled_full=y_all_s,\n",
    "        yhat_test_scaled=yhat_test_scaled,\n",
    "        dataset_seed=dataset_seed,\n",
    "    )\n",
    "\n",
    "\n",
    "# Main\n",
    "if MODE == \"train\":\n",
    "    for seed in range(\n",
    "        EXPERIMENT_TRIALS_CONFIG[\"seed_range_start\"],\n",
    "        EXPERIMENT_TRIALS_CONFIG[\"seed_range_end\"],\n",
    "    ):\n",
    "        print(f\"\\nStarting TFT STORE-level {TAG} training for seed {seed}\")\n",
    "        model_name = train_one_seed(seed)\n",
    "\n",
    "        official_eval_and_save(dataset_seed=seed, model_name=model_name)\n",
    "\n",
    "        clear_gpu_memory()\n",
    "        print(f\"Completed seed {seed}\")\n",
    "\n",
    "    print(f\"\\nAll TFT STORE-level {TAG} experiments completed!\")\n",
    "    print(\"All done.\")\n",
    "\n",
    "elif MODE == \"infer_plot\":\n",
    "    DATASET_SEED = 1\n",
    "    model_name = f\"tft_favorita_store_paperWindow_{TAG_LOWER}_seed{DATASET_SEED}_noFutureCov_staticOH_oilOnly_addRelIdx_BATCH_oilStd\"\n",
    "    official_eval_and_save(dataset_seed=DATASET_SEED, model_name=model_name)\n",
    "    print(\"\\nAll done.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODE: {MODE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
