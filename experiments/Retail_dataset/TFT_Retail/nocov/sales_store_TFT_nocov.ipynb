{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df605bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangxc1117/miniconda3/envs/geospatial-neural-adapter/lib/python3.10/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded spatial_utils from: /home/wangxc1117/geospatial-neural-adapter/geospatial_neural_adapter/cpp_extensions/spatial_utils.so\n",
      "Using CUDA: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   Memory: 8.6 GB\n",
      "\n",
      "=== TFT STORE Y-ONLY (rel-index only): input=90, output=30 ===\n",
      "No past cov, no future cov, no static cov, no calendar encoders.\n",
      "Model uses add_relative_index=True only.\n",
      "\n",
      "=== Loading train.csv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_561313/541830017.py:140: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(TRAIN_PATH)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ALL 53 stores | T_all(days)=395\n",
      "targets_full shape: (395, 53)\n",
      "\n",
      "=== OFFICIAL split lengths ===\n",
      "train: 335 | val: 30 | test: 30\n",
      "\n",
      "Built 53 store TimeSeries (TARGET ONLY).\n",
      "\n",
      "=== INTERNAL validation ===\n",
      "Train len = 335\n",
      "IntVal len = 240 | should be >> 120\n",
      "Experiment Configuration:\n",
      "  Trials per seed: 1\n",
      "  Dataset seeds: 1 to 1\n",
      "  Total experiments: 1\n",
      "  Device: GPU\n",
      "\n",
      "Starting TFT STORE-level H30 Y-ONLY relIdx training for seed 1\n",
      "\n",
      "[Seed 1] Training TFT Y-ONLY (add_relative_index=True) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                              | Type                             | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0      | train\n",
      "1  | val_metrics                       | MetricCollection                 | 0      | train\n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0      | train\n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0      | train\n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 3.5 K  | train\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 1.6 K  | train\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K | train\n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K | train\n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K | train\n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K | train\n",
      "10 | lstm_encoder                      | LSTM                             | 33.3 K | train\n",
      "11 | lstm_decoder                      | LSTM                             | 33.3 K | train\n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K  | train\n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K | train\n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K | train\n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K  | train\n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K | train\n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K  | train\n",
      "18 | output_layer                      | Linear                           | 1.1 K  | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "213 K     Trainable params\n",
      "0         Non-trainable params\n",
      "213 K     Total params\n",
      "0.853     Total estimated model params size (MB)\n",
      "172       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d049e246466418ba8f99293066e628b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a80744c6baf475b9ddf98c579f1f0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e39d024da3945c1a79f4fb661aa9e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa03c5220d849ce9b7b3deef20397a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd837add0db4816a3b87c47a301601a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc7aa0c353647e28092265a5dc7c0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db2b5f39dcf4d0b85ccc9cdb5dfafb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2078d6788124cb28f9e2e3464980c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf98e09ab80d470eb216ac5362eccdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded (best by INTERNAL val_loss).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TFT_STORE_H30 Y-ONLY relIdx OFFICIAL eval ===\n",
      "Val  RMSE=0.206386, MAE=0.157851, R2=-1.951542\n",
      "Test RMSE=0.217511, MAE=0.173511, R2=-2.414518\n",
      "\n",
      "=== P50 q-risk (Val) ===\n",
      "qrisk_p50_scaled_log = 0.406101\n",
      "qrisk_p50_unscaled_log = 0.034699\n",
      "qrisk_p50_sales = 0.296509\n",
      "\n",
      "=== P50 q-risk (Test) ===\n",
      "qrisk_p50_scaled_log = 0.517388\n",
      "qrisk_p50_unscaled_log = 0.038758\n",
      "qrisk_p50_sales = 0.427365\n",
      "[plot_tft_loss_curve] Saved -> /home/wangxc1117/TFTModel-use/geospatial-neural-adapter-dev/examples/try/use_admm_crood/sales_forecasting/TFT/sales_TFT_ADMM/store_level/compare_cov/nocov/tft_loss_favorita_store_h30_seed1_YONLY_relIdx_BATCH.png\n",
      "\n",
      "Start plotting ALL 53 stores ...\n",
      "All figures saved under: /home/wangxc1117/TFTModel-use/geospatial-neural-adapter-dev/examples/try/use_admm_crood/sales_forecasting/TFT/sales_TFT_ADMM/store_level/compare_cov/nocov/TFT_plots_favorita_store_paperWindow_h30_YONLY_relIdx_BATCH\n",
      "Completed seed 1\n",
      "\n",
      "All TFT STORE-level H30 Y-ONLY relIdx experiments completed!\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    clear_gpu_memory,\n",
    "    create_experiment_config,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling\n",
    "\n",
    "\n",
    "# Global settings & dirs\n",
    "MODE = \"train\"\n",
    "\n",
    "try:\n",
    "    EXP_ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    EXP_ROOT = Path.cwd()\n",
    "\n",
    "CKPT_DIR = (EXP_ROOT / \"darts_ckpt_favorita_store_paperWindow_h30_YONLY_relIdx_BATCH\").resolve()\n",
    "RUNS_DIR = (EXP_ROOT / \"TFT_runs_favorita_store_paperWindow_h30_YONLY_relIdx_BATCH\").resolve()\n",
    "PLOTS_DIR = (EXP_ROOT / \"TFT_plots_favorita_store_paperWindow_h30_YONLY_relIdx_BATCH\").resolve()\n",
    "\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info[\"device\"] == \"cuda\":\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "\n",
    "# TFT config\n",
    "TREND_CONFIG: Dict[str, Any] = {\n",
    "    \"input_chunk_length\": 90,\n",
    "    \"output_chunk_length\": 30,\n",
    "    \"n_epochs\": 20,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_size\": 32,\n",
    "    \"optimizer_kwargs\": {\"lr\": 3e-4},\n",
    "    \"random_state\": GLOBAL_SEED,\n",
    "    \"force_reset\": True,\n",
    "    \"full_attention\": False,\n",
    "    \"add_relative_index\": True,\n",
    "    \"pl_trainer_kwargs\": {\n",
    "        \"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"devices\": 1,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"enable_model_summary\": False,\n",
    "        \"enable_checkpointing\": True,\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "        \"callbacks\": [\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "                patience=5,\n",
    "                min_delta=5e-4,\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "L = int(TREND_CONFIG[\"input_chunk_length\"])\n",
    "H = int(TREND_CONFIG[\"output_chunk_length\"])\n",
    "TAG = f\"H{H}\"\n",
    "TAG_LOWER = f\"h{H}\"\n",
    "\n",
    "print(f\"\\n=== TFT STORE Y-ONLY (rel-index only): input={L}, output={H} ===\")\n",
    "print(\"No past cov, no future cov, no static cov, no calendar encoders.\")\n",
    "print(\"Model uses add_relative_index=True only.\")\n",
    "\n",
    "\n",
    "# Paper window (no date prints)\n",
    "PAPER_TRAIN_START = pd.Timestamp(\"2015-01-01\")\n",
    "PAPER_TRAIN_END = pd.Timestamp(\"2015-12-01\")\n",
    "PAPER_VAL_DAYS = 30\n",
    "PAPER_TEST_DAYS = 30\n",
    "\n",
    "PAPER_VAL_START = PAPER_TRAIN_END + pd.Timedelta(days=1)\n",
    "PAPER_VAL_END = PAPER_VAL_START + pd.Timedelta(days=PAPER_VAL_DAYS - 1)\n",
    "PAPER_TEST_START = PAPER_VAL_END + pd.Timedelta(days=1)\n",
    "PAPER_TEST_END = PAPER_TEST_START + pd.Timedelta(days=PAPER_TEST_DAYS - 1)\n",
    "PAPER_ALL_START = PAPER_TRAIN_START\n",
    "PAPER_ALL_END = PAPER_TEST_END\n",
    "\n",
    "\n",
    "# Load train.csv\n",
    "DATA_ROOT = Path(\"/home/wangxc1117/experiment_data/sales_forecasting_data\")\n",
    "TRAIN_PATH = DATA_ROOT / \"train.csv\"\n",
    "if not TRAIN_PATH.exists():\n",
    "    raise FileNotFoundError(\"train.csv not found at configured path.\")\n",
    "\n",
    "print(\"\\n=== Loading train.csv ===\")\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"onpromotion\"] = df[\"onpromotion\"].fillna(0).astype(int)\n",
    "df = df[(df[\"date\"] >= PAPER_ALL_START) & (df[\"date\"] <= PAPER_ALL_END)].copy()\n",
    "\n",
    "\n",
    "# Store-level daily sales (target only)\n",
    "df_store_sales = (\n",
    "    df.groupby([\"date\", \"store_nbr\"], as_index=False)[\"unit_sales\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"unit_sales\": \"store_sales\"})\n",
    ")\n",
    "df_store_sales[\"store_sales\"] = df_store_sales[\"store_sales\"].clip(lower=0.0)\n",
    "\n",
    "stores = sorted(df_store_sales[\"store_nbr\"].unique())\n",
    "date_index = pd.date_range(start=PAPER_ALL_START, end=PAPER_ALL_END, freq=\"D\")\n",
    "T_all = len(date_index)\n",
    "N = len(stores)\n",
    "\n",
    "print(f\"\\nUsing ALL {N} stores | T_all(days)={T_all}\")\n",
    "\n",
    "full_idx = pd.MultiIndex.from_product([date_index, stores], names=[\"date\", \"store_nbr\"])\n",
    "\n",
    "panel_sales_raw = (\n",
    "    df_store_sales.set_index([\"date\", \"store_nbr\"])\n",
    "    .reindex(full_idx)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "panel_sales = panel_sales_raw.copy()\n",
    "panel_sales[\"store_sales\"] = panel_sales.groupby(\"store_nbr\")[\"store_sales\"].ffill()\n",
    "panel_sales.loc[panel_sales[\"store_sales\"].isna(), \"store_sales\"] = 0.0\n",
    "panel_sales[\"log_sales\"] = np.log1p(panel_sales[\"store_sales\"]).astype(\"float32\")\n",
    "\n",
    "Y_df = (\n",
    "    panel_sales[\"log_sales\"]\n",
    "    .unstack(\"store_nbr\")\n",
    "    .reindex(index=date_index, columns=stores)\n",
    "    .astype(\"float32\")\n",
    ")\n",
    "targets_full = Y_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "print(\"targets_full shape:\", targets_full.shape)\n",
    "\n",
    "\n",
    "# Official split lengths (no dates)\n",
    "cut_train = int((PAPER_TRAIN_END - PAPER_ALL_START).days + 1)\n",
    "cut_val = cut_train + PAPER_VAL_DAYS\n",
    "assert cut_val + PAPER_TEST_DAYS == T_all\n",
    "\n",
    "print(\"\\n=== OFFICIAL split lengths ===\")\n",
    "print(\"train:\", cut_train, \"| val:\", (cut_val - cut_train), \"| test:\", (T_all - cut_val))\n",
    "\n",
    "\n",
    "# Scaling targets only (fit on train only)\n",
    "cat_dummy = np.zeros((T_all, N, 1), dtype=np.int64)\n",
    "cont_dummy = np.zeros((T_all, N, 1), dtype=np.float32)\n",
    "\n",
    "train_ratio = cut_train / T_all\n",
    "val_ratio = PAPER_VAL_DAYS / T_all\n",
    "\n",
    "train_dataset_scl, val_dataset_scl, test_dataset_scl, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_dummy,\n",
    "    cont_features=cont_dummy,\n",
    "    targets=targets_full,\n",
    "    train_ratio=train_ratio,\n",
    "    val_ratio=val_ratio,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def stitch_y(dsets) -> np.ndarray:\n",
    "    ys = [ds.tensors[2].cpu().numpy().astype(np.float32) for ds in dsets]\n",
    "    return np.concatenate(ys, axis=0)\n",
    "\n",
    "\n",
    "y_all_s = stitch_y([train_dataset_scl, val_dataset_scl, test_dataset_scl])\n",
    "assert y_all_s.shape == targets_full.shape\n",
    "\n",
    "\n",
    "# q-risk helpers\n",
    "def quantile_loss(y_true: np.ndarray, y_pred: np.ndarray, q: float) -> np.ndarray:\n",
    "    e = y_true - y_pred\n",
    "    return np.maximum(q * e, (q - 1.0) * e)\n",
    "\n",
    "\n",
    "def qrisk(y_true: np.ndarray, y_pred: np.ndarray, q: float, eps: float = 1e-8) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    num = 2.0 * np.sum(quantile_loss(y_true, y_pred, q))\n",
    "    den = np.sum(np.abs(y_true)) + eps\n",
    "    return float(num / den)\n",
    "\n",
    "\n",
    "def _try_get_target_scaler_params(prep) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    if prep is None:\n",
    "        return None, None\n",
    "    ts = getattr(prep, \"target_scaler\", None)\n",
    "    if ts is None:\n",
    "        return None, None\n",
    "    mean_ = getattr(ts, \"mean_\", None)\n",
    "    scale_ = getattr(ts, \"scale_\", None)\n",
    "    if mean_ is None or scale_ is None:\n",
    "        return None, None\n",
    "    return np.asarray(mean_, dtype=np.float64), np.asarray(scale_, dtype=np.float64)\n",
    "\n",
    "\n",
    "TARGET_MEAN, TARGET_SCALE = _try_get_target_scaler_params(preprocessor)\n",
    "\n",
    "\n",
    "def inverse_scale_log(y_scaled: np.ndarray) -> Optional[np.ndarray]:\n",
    "    if TARGET_MEAN is None or TARGET_SCALE is None:\n",
    "        return None\n",
    "    y_scaled = np.asarray(y_scaled, dtype=np.float64)\n",
    "\n",
    "    m = TARGET_MEAN\n",
    "    s = TARGET_SCALE\n",
    "\n",
    "    if m.ndim == 1 and m.shape[0] == y_scaled.shape[1]:\n",
    "        m2 = m.reshape(1, -1)\n",
    "    elif m.size == 1:\n",
    "        m2 = np.full((1, y_scaled.shape[1]), float(m.reshape(-1)[0]), dtype=np.float64)\n",
    "    else:\n",
    "        m2 = m.reshape(1, -1) if m.ndim == 1 else m\n",
    "\n",
    "    if s.ndim == 1 and s.shape[0] == y_scaled.shape[1]:\n",
    "        s2 = s.reshape(1, -1)\n",
    "    elif s.size == 1:\n",
    "        s2 = np.full((1, y_scaled.shape[1]), float(s.reshape(-1)[0]), dtype=np.float64)\n",
    "    else:\n",
    "        s2 = s.reshape(1, -1) if s.ndim == 1 else s\n",
    "\n",
    "    return (y_scaled * s2 + m2).astype(np.float32)\n",
    "\n",
    "\n",
    "def log_to_sales(y_log: np.ndarray) -> np.ndarray:\n",
    "    y = np.expm1(np.asarray(y_log, dtype=np.float64))\n",
    "    y = np.clip(y, 0.0, None)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_p50_qrisk_block(y_true_2d: np.ndarray, y_pred_2d: np.ndarray) -> Dict[str, float]:\n",
    "    out: Dict[str, float] = {}\n",
    "    out[\"qrisk_p50_scaled_log\"] = qrisk(y_true_2d, y_pred_2d, q=0.5)\n",
    "\n",
    "    y_true_log = inverse_scale_log(y_true_2d)\n",
    "    y_pred_log = inverse_scale_log(y_pred_2d)\n",
    "    if y_true_log is not None and y_pred_log is not None:\n",
    "        out[\"qrisk_p50_unscaled_log\"] = qrisk(y_true_log, y_pred_log, q=0.5)\n",
    "        out[\"qrisk_p50_sales\"] = qrisk(log_to_sales(y_true_log), log_to_sales(y_pred_log), q=0.5)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Build TimeSeries lists (target only)\n",
    "series_all: List[TimeSeries] = []\n",
    "for j, sid in enumerate(stores):\n",
    "    name = f\"store_{sid}\"\n",
    "    ts = TimeSeries.from_times_and_values(\n",
    "        times=date_index,\n",
    "        values=y_all_s[:, j:j + 1],\n",
    "        columns=[name],\n",
    "        freq=\"D\",\n",
    "    )\n",
    "    series_all.append(ts)\n",
    "\n",
    "print(f\"\\nBuilt {len(series_all)} store TimeSeries (TARGET ONLY).\")\n",
    "\n",
    "\n",
    "# Internal validation\n",
    "INTERNAL_VAL_DAYS = 240\n",
    "\n",
    "\n",
    "def slice_list(ts_list: List[TimeSeries], a: int, b: int) -> List[TimeSeries]:\n",
    "    return [ts[a:b] for ts in ts_list]\n",
    "\n",
    "\n",
    "train_series_in = slice_list(series_all, 0, cut_train)\n",
    "\n",
    "internal_val_start = max(0, cut_train - INTERNAL_VAL_DAYS)\n",
    "internal_val_end = cut_train\n",
    "min_needed = L + H\n",
    "if (internal_val_end - internal_val_start) < min_needed:\n",
    "    internal_val_start = max(0, internal_val_end - min_needed)\n",
    "\n",
    "val_series_in = slice_list(series_all, internal_val_start, internal_val_end)\n",
    "\n",
    "print(\"\\n=== INTERNAL validation ===\")\n",
    "print(\"Train len =\", len(train_series_in[0]))\n",
    "print(\"IntVal len =\", len(val_series_in[0]), \"| should be >>\", (L + H))\n",
    "\n",
    "\n",
    "# OFFICIAL inference (batch predict, target-only)\n",
    "def infer_official_direct_30_batch_yonly(tft: TFTModel) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ts_ctx_val_list = slice_list(series_all, 0, cut_train)\n",
    "    pred_val_list = tft.predict(n=H, series=ts_ctx_val_list, verbose=False)\n",
    "    if isinstance(pred_val_list, TimeSeries):\n",
    "        pred_val_list = [pred_val_list]\n",
    "    if len(pred_val_list) != N:\n",
    "        raise RuntimeError(f\"[Val] expected {N} predictions, got {len(pred_val_list)}\")\n",
    "\n",
    "    yhat_val = np.full((PAPER_VAL_DAYS, N), np.nan, dtype=np.float32)\n",
    "    for j, pred in enumerate(pred_val_list):\n",
    "        if len(pred) != PAPER_VAL_DAYS:\n",
    "            raise RuntimeError(f\"[Val] store idx {j}: expected 30 preds, got {len(pred)}\")\n",
    "        yhat_val[:, j] = pred.values(copy=False).astype(np.float32)[:, 0]\n",
    "\n",
    "    ts_ctx_test_list = slice_list(series_all, 0, cut_val)\n",
    "    pred_test_list = tft.predict(n=H, series=ts_ctx_test_list, verbose=False)\n",
    "    if isinstance(pred_test_list, TimeSeries):\n",
    "        pred_test_list = [pred_test_list]\n",
    "    if len(pred_test_list) != N:\n",
    "        raise RuntimeError(f\"[Test] expected {N} predictions, got {len(pred_test_list)}\")\n",
    "\n",
    "    yhat_test = np.full((PAPER_TEST_DAYS, N), np.nan, dtype=np.float32)\n",
    "    for j, pred in enumerate(pred_test_list):\n",
    "        if len(pred) != PAPER_TEST_DAYS:\n",
    "            raise RuntimeError(f\"[Test] store idx {j}: expected 30 preds, got {len(pred)}\")\n",
    "        yhat_test[:, j] = pred.values(copy=False).astype(np.float32)[:, 0]\n",
    "\n",
    "    if np.isnan(yhat_val).any() or np.isnan(yhat_test).any():\n",
    "        raise RuntimeError(\"Inference produced NaNs in yhat_val/yhat_test.\")\n",
    "\n",
    "    return yhat_val, yhat_test\n",
    "\n",
    "\n",
    "# Loss curve plot\n",
    "def find_metrics_csv(base_dir: Path, dataset_seed: int, model_name: str) -> Path:\n",
    "    base = Path(base_dir) / f\"TFT_Favorita_seed_{dataset_seed}\" / model_name\n",
    "    pattern = str(base / \"version_*\" / \"metrics.csv\")\n",
    "    candidates = glob.glob(pattern)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"metrics.csv not found.\")\n",
    "    latest = max(candidates, key=lambda p: Path(p).stat().st_mtime)\n",
    "    return Path(latest).resolve()\n",
    "\n",
    "\n",
    "def plot_tft_loss_curve(metrics_csv_path: Path, out_path: Path):\n",
    "    if not metrics_csv_path.exists():\n",
    "        print(\"[plot_tft_loss_curve] metrics.csv not found, skip.\")\n",
    "        return\n",
    "    dfm = pd.read_csv(metrics_csv_path)\n",
    "    dfm.columns = [c.replace(\"/\", \"_\").replace(\".\", \"_\") for c in dfm.columns]\n",
    "    if \"epoch\" not in dfm.columns:\n",
    "        print(\"[plot_tft_loss_curve] no epoch col, skip.\")\n",
    "        return\n",
    "\n",
    "    loss_cols = [c for c in dfm.columns if \"loss\" in c.lower() and pd.api.types.is_numeric_dtype(dfm[c])]\n",
    "    if not loss_cols:\n",
    "        print(\"[plot_tft_loss_curve] no numeric loss col, skip.\")\n",
    "        return\n",
    "\n",
    "    grp = dfm[[\"epoch\"] + loss_cols].groupby(\"epoch\", as_index=False).mean(numeric_only=True)\n",
    "    train_col = \"train_loss\" if \"train_loss\" in grp.columns else (\"loss\" if \"loss\" in grp.columns else None)\n",
    "    val_col = next((c for c in grp.columns if \"loss\" in c.lower() and \"val\" in c.lower()), None)\n",
    "\n",
    "    plt.figure(figsize=(8, 4), dpi=140)\n",
    "    if train_col is not None:\n",
    "        plt.plot(grp[\"epoch\"], grp[train_col], \"-\", linewidth=2, label=\"train_loss\")\n",
    "    if val_col is not None:\n",
    "        plt.plot(grp[\"epoch\"], grp[val_col], \"-o\", linewidth=2, markersize=4, label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"TFT loss vs epoch (internal val)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "    print(f\"[plot_tft_loss_curve] Saved -> {out_path}\")\n",
    "\n",
    "\n",
    "# Plotting\n",
    "def plot_store_two_figs(\n",
    "    sid: int,\n",
    "    y_true_scaled_full: np.ndarray,\n",
    "    yhat_test_scaled: np.ndarray,\n",
    "    out_dir: Path,\n",
    "    dataset_seed: int,\n",
    "):\n",
    "    j = stores.index(sid)\n",
    "\n",
    "    y_val_true = y_true_scaled_full[cut_train:cut_val, j]\n",
    "    y_test_true = y_true_scaled_full[cut_val:, j]\n",
    "    y_test_pred = yhat_test_scaled[:, j]\n",
    "\n",
    "    dates_val = date_index[cut_train:cut_val]\n",
    "    dates_test = date_index[cut_val:]\n",
    "    dates_true_vt = dates_val.append(dates_test)\n",
    "    y_true_vt = np.concatenate([y_val_true, y_test_true], axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=140)\n",
    "    plt.plot(dates_true_vt, y_true_vt, \"-\", linewidth=2.0, color=\"k\", label=\"True (Val+Test)\")\n",
    "    plt.plot(dates_test, y_test_pred, \"-\", linewidth=1.8, color=\"C3\", label=\"Test Pred\")\n",
    "    plt.axvline(date_index[cut_train], linestyle=\"--\", linewidth=1, label=\"train/val split\")\n",
    "    plt.axvline(date_index[cut_val], linestyle=\"--\", linewidth=1, label=\"val/test split\")\n",
    "    plt.title(f\"FIG1 {TAG} store {sid}: True + Test Pred\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Scaled log_sales\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    p1 = out_dir / f\"FIG1_store{sid}_{TAG_LOWER}_seed{dataset_seed}.png\"\n",
    "    plt.savefig(p1)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=140)\n",
    "    y_all_true = y_true_scaled_full[:, j]\n",
    "    plt.plot(date_index, y_all_true, \"-\", linewidth=1.8, color=\"k\", label=\"All True\")\n",
    "    plt.plot(dates_test, y_test_pred, \"-\", linewidth=1.8, color=\"C3\", label=\"Test Pred\")\n",
    "    plt.axvline(date_index[cut_train], linestyle=\"--\", linewidth=1, label=\"train/val split\")\n",
    "    plt.axvline(date_index[cut_val], linestyle=\"--\", linewidth=1, label=\"val/test split\")\n",
    "    plt.title(f\"FIG2 {TAG} store {sid}: All True + Test Pred\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Scaled log_sales\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    p2 = out_dir / f\"FIG2_store{sid}_{TAG_LOWER}_seed{dataset_seed}.png\"\n",
    "    plt.savefig(p2)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_all_stores_two_figs(y_true_scaled_full: np.ndarray, yhat_test_scaled: np.ndarray, dataset_seed: int):\n",
    "    print(f\"\\nStart plotting ALL {len(stores)} stores ...\")\n",
    "    for sid in stores:\n",
    "        plot_store_two_figs(\n",
    "            sid=sid,\n",
    "            y_true_scaled_full=y_true_scaled_full,\n",
    "            yhat_test_scaled=yhat_test_scaled,\n",
    "            out_dir=PLOTS_DIR,\n",
    "            dataset_seed=dataset_seed,\n",
    "        )\n",
    "    print(f\"All figures saved under: {PLOTS_DIR}\")\n",
    "\n",
    "\n",
    "# Experiment runner\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=1,\n",
    "    n_dataset_seeds=1,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=2,\n",
    ")\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "\n",
    "\n",
    "def build_model(model_name: str, log_root: Path) -> TFTModel:\n",
    "    pl_kwargs = dict(TREND_CONFIG[\"pl_trainer_kwargs\"])\n",
    "    pl_kwargs[\"logger\"] = CSVLogger(save_dir=str(log_root), name=model_name)\n",
    "\n",
    "    return TFTModel(\n",
    "        input_chunk_length=L,\n",
    "        output_chunk_length=H,\n",
    "        n_epochs=TREND_CONFIG[\"n_epochs\"],\n",
    "        hidden_size=TREND_CONFIG[\"hidden_size\"],\n",
    "        num_attention_heads=TREND_CONFIG[\"num_attention_heads\"],\n",
    "        dropout=TREND_CONFIG[\"dropout\"],\n",
    "        batch_size=TREND_CONFIG[\"batch_size\"],\n",
    "        random_state=TREND_CONFIG[\"random_state\"],\n",
    "        force_reset=TREND_CONFIG[\"force_reset\"],\n",
    "        full_attention=TREND_CONFIG[\"full_attention\"],\n",
    "        add_relative_index=TREND_CONFIG[\"add_relative_index\"],\n",
    "        pl_trainer_kwargs=pl_kwargs,\n",
    "        model_name=model_name,\n",
    "        work_dir=str(CKPT_DIR),\n",
    "        save_checkpoints=True,\n",
    "        optimizer_kwargs=TREND_CONFIG.get(\"optimizer_kwargs\", None),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_one_seed(dataset_seed: int) -> str:\n",
    "    np.random.seed(dataset_seed)\n",
    "    torch.manual_seed(dataset_seed)\n",
    "\n",
    "    log_root = RUNS_DIR / f\"TFT_Favorita_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_name = f\"tft_favorita_store_paperWindow_{TAG_LOWER}_seed{dataset_seed}_YONLY_relIdx_BATCH\"\n",
    "\n",
    "    print(f\"\\n[Seed {dataset_seed}] Training TFT Y-ONLY (add_relative_index=True) ...\")\n",
    "\n",
    "    tft = build_model(model_name=model_name, log_root=log_root)\n",
    "\n",
    "    tft.fit(\n",
    "        series=train_series_in,\n",
    "        val_series=val_series_in,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return model_name\n",
    "\n",
    "\n",
    "def load_best_ckpt(model_name: str) -> TFTModel:\n",
    "    tft = TFTModel.load_from_checkpoint(model_name=model_name, work_dir=str(CKPT_DIR), best=True)\n",
    "    print(\"Checkpoint loaded (best by INTERNAL val_loss).\")\n",
    "    return tft\n",
    "\n",
    "\n",
    "def official_eval_and_save(dataset_seed: int, model_name: str) -> None:\n",
    "    tft = load_best_ckpt(model_name=model_name)\n",
    "\n",
    "    yhat_val_scaled, yhat_test_scaled = infer_official_direct_30_batch_yonly(tft)\n",
    "\n",
    "    y_val_true = y_all_s[cut_train:cut_val, :]\n",
    "    y_test_true = y_all_s[cut_val:, :]\n",
    "\n",
    "    val_y_t = torch.from_numpy(y_val_true).to(DEVICE)\n",
    "    test_y_t = torch.from_numpy(y_test_true).to(DEVICE)\n",
    "    yhat_val_t = torch.from_numpy(yhat_val_scaled).to(DEVICE)\n",
    "    yhat_test_t = torch.from_numpy(yhat_test_scaled).to(DEVICE)\n",
    "\n",
    "    rmse_v, mae_v, r2_v = compute_metrics(val_y_t, yhat_val_t)\n",
    "    rmse_t, mae_t, r2_t = compute_metrics(test_y_t, yhat_test_t)\n",
    "\n",
    "    qrisk_val = compute_p50_qrisk_block(y_val_true, yhat_val_scaled)\n",
    "    qrisk_test = compute_p50_qrisk_block(y_test_true, yhat_test_scaled)\n",
    "\n",
    "    print(f\"\\n=== TFT_STORE_{TAG} Y-ONLY relIdx OFFICIAL eval ===\")\n",
    "    print(f\"Val  RMSE={rmse_v:.6f}, MAE={mae_v:.6f}, R2={r2_v:.6f}\")\n",
    "    print(f\"Test RMSE={rmse_t:.6f}, MAE={mae_t:.6f}, R2={r2_t:.6f}\")\n",
    "\n",
    "    print(\"\\n=== P50 q-risk (Val) ===\")\n",
    "    for k, v in qrisk_val.items():\n",
    "        print(f\"{k} = {v:.6f}\")\n",
    "\n",
    "    print(\"\\n=== P50 q-risk (Test) ===\")\n",
    "    for k, v in qrisk_test.items():\n",
    "        print(f\"{k} = {v:.6f}\")\n",
    "\n",
    "    csv_path = EXP_ROOT / f\"metrics_summary_TFT_Favorita_STORE_{TAG}_paperWindow_YONLY_relIdx_BATCH.csv\"\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\n",
    "                \"seed\", \"model\",\n",
    "                \"rmse_val\", \"mae_val\", \"r2_val\",\n",
    "                \"rmse_test\", \"mae_test\", \"r2_test\",\n",
    "                \"val_qrisk_p50_scaled_log\", \"val_qrisk_p50_unscaled_log\", \"val_qrisk_p50_sales\",\n",
    "                \"test_qrisk_p50_scaled_log\", \"test_qrisk_p50_unscaled_log\", \"test_qrisk_p50_sales\",\n",
    "            ])\n",
    "\n",
    "        model_label = f\"TFT_STORE_{TAG}_paperWindow_YONLY_relIdx_BATCH\"\n",
    "        w.writerow([\n",
    "            dataset_seed, model_label,\n",
    "            float(rmse_v), float(mae_v), float(r2_v),\n",
    "            float(rmse_t), float(mae_t), float(r2_t),\n",
    "            float(qrisk_val[\"qrisk_p50_scaled_log\"]),\n",
    "            float(qrisk_val.get(\"qrisk_p50_unscaled_log\", np.nan)),\n",
    "            float(qrisk_val.get(\"qrisk_p50_sales\", np.nan)),\n",
    "            float(qrisk_test[\"qrisk_p50_scaled_log\"]),\n",
    "            float(qrisk_test.get(\"qrisk_p50_unscaled_log\", np.nan)),\n",
    "            float(qrisk_test.get(\"qrisk_p50_sales\", np.nan)),\n",
    "        ])\n",
    "\n",
    "    try:\n",
    "        metrics_csv_path = find_metrics_csv(RUNS_DIR, dataset_seed=dataset_seed, model_name=model_name)\n",
    "        loss_png = EXP_ROOT / f\"tft_loss_favorita_store_{TAG_LOWER}_seed{dataset_seed}_YONLY_relIdx_BATCH.png\"\n",
    "        plot_tft_loss_curve(metrics_csv_path, loss_png)\n",
    "    except Exception as e:\n",
    "        print(\"[Loss plot] skipped due to:\", e)\n",
    "\n",
    "    plot_all_stores_two_figs(\n",
    "        y_true_scaled_full=y_all_s,\n",
    "        yhat_test_scaled=yhat_test_scaled,\n",
    "        dataset_seed=dataset_seed,\n",
    "    )\n",
    "\n",
    "\n",
    "# Main\n",
    "if MODE == \"train\":\n",
    "    for seed in range(\n",
    "        EXPERIMENT_TRIALS_CONFIG[\"seed_range_start\"],\n",
    "        EXPERIMENT_TRIALS_CONFIG[\"seed_range_end\"],\n",
    "    ):\n",
    "        print(f\"\\nStarting TFT STORE-level {TAG} Y-ONLY relIdx training for seed {seed}\")\n",
    "        model_name = train_one_seed(seed)\n",
    "        official_eval_and_save(dataset_seed=seed, model_name=model_name)\n",
    "        clear_gpu_memory()\n",
    "        print(f\"Completed seed {seed}\")\n",
    "\n",
    "    print(f\"\\nAll TFT STORE-level {TAG} Y-ONLY relIdx experiments completed!\")\n",
    "    print(\"All done.\")\n",
    "\n",
    "elif MODE == \"infer_plot\":\n",
    "    DATASET_SEED = 1\n",
    "    model_name = f\"tft_favorita_store_paperWindow_{TAG_LOWER}_seed{DATASET_SEED}_YONLY_relIdx_BATCH\"\n",
    "    official_eval_and_save(dataset_seed=DATASET_SEED, model_name=model_name)\n",
    "    print(\"\\nAll done.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODE: {MODE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
