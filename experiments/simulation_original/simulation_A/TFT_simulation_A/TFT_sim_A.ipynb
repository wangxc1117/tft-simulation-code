{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangxc1117/miniconda3/envs/geospatial-neural-adapter/lib/python3.10/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded spatial_utils from: /home/wangxc1117/geospatial-neural-adapter/geospatial_neural_adapter/cpp_extensions/spatial_utils.so\n",
      "Device: cuda\n",
      "Synthetic y shape: (1500, 36)\n",
      "Synthetic cont shape: (1500, 36, 3)\n",
      "Total time steps: 1500\n",
      "Number of points: 36\n",
      "Freq: 1H\n",
      "\n",
      "=== Split (by time index) ===\n",
      "Train: 0 -> 1049 | len = 1050\n",
      "Val  : 1050 -> 1274 | len = 225\n",
      "Test : 1275 -> 1499 | len = 225\n",
      "\n",
      "=== After pooled scaling (train-only) ===\n",
      "Y_scaled: (1500, 36) | finite: True\n",
      "X_scaled: (1500, 36, 3) | finite: True\n",
      "\n",
      "Built TimeSeries:\n",
      "Targets: 36\n",
      "Past covs: 36\n",
      "\n",
      "=== INTERNAL validation (for early stopping only) ===\n",
      "Train len: 1050\n",
      "IntVal idx: 810 -> 1049 | len = 240\n",
      "IntVal time: 2000-02-03 18:00:00 -> 2000-02-13 17:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training TFT (Scenario A: linear world, with past cov) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                              | Type                             | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0      | train\n",
      "1  | val_metrics                       | MetricCollection                 | 0      | train\n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0      | train\n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 5.0 K  | train\n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 8.8 K  | train\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 1.6 K  | train\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 16.8 K | train\n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 16.8 K | train\n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 16.8 K | train\n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 16.8 K | train\n",
      "10 | lstm_encoder                      | LSTM                             | 33.3 K | train\n",
      "11 | lstm_decoder                      | LSTM                             | 33.3 K | train\n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 8.4 K  | train\n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 20.9 K | train\n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 10.4 K | train\n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 8.4 K  | train\n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 16.8 K | train\n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 8.4 K  | train\n",
      "18 | output_layer                      | Linear                           | 1.1 K  | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "223 K     Trainable params\n",
      "0         Non-trainable params\n",
      "223 K     Total params\n",
      "0.894     Total estimated model params size (MB)\n",
      "276       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c83dfc76ae6452d8a3d3a2110acfcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084db784631f4ce3893c0056fd397a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b20c7a76c40411e919a44d5d99cc780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69529937f6f46d68491951de57501b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7775d4fc83402f9e5e343501e08589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fa0306fddf451fa2eb342c471b6279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15cd749e7d14c14ae830dc48f5b3ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dec465286044c6a15e197ab166633e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638436b2700d4b8f9a0c35310d020820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fd6206cc364a6f90431a2056bcae17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2590781920e46c0b6125a5bcad83fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9763e6acd49749a287c934565fde5fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19271897d8884af19994f24b05123c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2913f964b1e04683b92989b51bd08882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf61e21585aa4863bad57493e4dade62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc05ea9c9ad8442bbe95aaae8889d44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00da2fc1a5234a25b430ac62f53bff3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3e694f709a4f32af44ad15d35f3206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e382958071407bb3e1a6d044786b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276d838a3b4f480dbb77bebbfbfc62b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6867e6d0becb4c86b5ab0a44d0327b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b0198a45124d3389b2ac4f8d4fbee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f2c3c07733415bacd4eac57310f6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e52cea452f246238556f20160dbc3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c37c921a2b40f2a094d1fa4d043bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86420d8e153445819e9b84c45c5bf97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f38232908dd41308422ebfc09068a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2892877389244fcad183991258a0c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57ef7824e5a4fbbb1c32fccd2033ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b7be140766439d9bd981369359d354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51b0bb643be488982007b9db36f8c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8d89aaa9ec49898f2537ed268fbbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading best checkpoint ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rolling non-overlap (POOLED, TEST) ===\n",
      "windows=9 | step=24 | each window predicts H=24 | points=36\n",
      "RMSE: 1.3267632722854614\n",
      "MAE : 1.0594276189804077\n",
      "\n",
      "=== P50 q-risk (ROLLING, TEST) ===\n",
      "scaled  : 0.8868321083770823\n",
      "unscaled: 0.015422872808764674\n",
      "\n",
      "=== Plotting 2 figs per grid point (Scenario A) ===\n",
      "Saved 2 figs/point for 36 points under: /home/wangxc1117/TFTModel-use/geospatial-neural-adapter-dev/examples/try/simulation/TFT_simulation_A/TFT_plots_synth_tft_A_2figs\n",
      "\n",
      "All done (Scenario A: linear world, TFT with past covariates).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "\n",
    "\n",
    "# Global settings & experiment dirs\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# output dirs\n",
    "try:\n",
    "    EXP_ROOT = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    EXP_ROOT = Path.cwd()\n",
    "\n",
    "CKPT_DIR = (EXP_ROOT / \"darts_ckpt_synth_tft_A\").resolve()\n",
    "RUNS_DIR = (EXP_ROOT / \"TFT_runs_synth_tft_A\").resolve()\n",
    "PLOTS_DIR = (EXP_ROOT / \"TFT_plots_synth_tft_A_2figs\").resolve()\n",
    "\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#  Generate synthetic data \n",
    "\n",
    "N_POINTS = 36         \n",
    "T_TOTAL = 1500         \n",
    "EIGENVALUE = 3.0      \n",
    "\n",
    "locs = np.linspace(-5.0, 5.0, N_POINTS).astype(np.float32)  \n",
    "\n",
    "cat_synth, cont_synth, y_synth = generate_time_synthetic_data(\n",
    "    locs=locs,\n",
    "    n_time_steps=T_TOTAL,\n",
    "    noise_std=0.3,\n",
    "    eigenvalue=EIGENVALUE,\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.0,\n",
    "    non_linear_strength=0.0,\n",
    "    seed=GLOBAL_SEED,\n",
    ")\n",
    "\n",
    "print(\"Synthetic y shape:\", y_synth.shape)\n",
    "print(\"Synthetic cont shape:\", cont_synth.shape)\n",
    "\n",
    "Y = y_synth.astype(np.float32)         \n",
    "X = cont_synth.astype(np.float32)       \n",
    "\n",
    "T_total, N = Y.shape\n",
    "_, _, P_time = X.shape\n",
    "\n",
    "time_start = pd.Timestamp(\"2000-01-01 00:00:00\")\n",
    "time_index = pd.date_range(start=time_start, periods=T_total, freq=\"1H\")\n",
    "freq = \"1H\"\n",
    "if len(time_index) != T_total:\n",
    "    raise RuntimeError(\"Global time_index length mismatch.\")\n",
    "\n",
    "print(\"Total time steps:\", T_total)\n",
    "print(\"Number of points:\", N)\n",
    "print(\"Freq:\", freq)\n",
    "\n",
    "PAST_COV_COLS = [\"f1\", \"f2\", \"f3\"]\n",
    "\n",
    "cat_dummy = np.zeros((T_total, N, 1), dtype=np.int64)\n",
    "\n",
    "\n",
    "# Train/Val/Test split + pooled scaling \n",
    "\n",
    "train_ratio = 0.70\n",
    "val_ratio = 0.15\n",
    "\n",
    "cut_train = int(T_total * train_ratio)\n",
    "cut_val = int(T_total * (train_ratio + val_ratio))\n",
    "if not (0 < cut_train < cut_val < T_total):\n",
    "    raise ValueError(\"Bad split indices computed from ratios.\")\n",
    "\n",
    "print(\"\\n=== Split (by time index) ===\")\n",
    "print(\"Train:\", 0, \"->\", cut_train - 1, \"| len =\", cut_train)\n",
    "print(\"Val  :\", cut_train, \"->\", cut_val - 1, \"| len =\", cut_val - cut_train)\n",
    "print(\"Test :\", cut_val, \"->\", T_total - 1, \"| len =\", T_total - cut_val)\n",
    "\n",
    "train_ds, val_ds, test_ds, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_dummy,\n",
    "    cont_features=X,\n",
    "    targets=Y,\n",
    "    train_ratio=train_ratio,\n",
    "    val_ratio=val_ratio,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def stitch_targets(dsets) -> np.ndarray:\n",
    "    return np.concatenate(\n",
    "        [ds.tensors[2].detach().cpu().numpy().astype(np.float32) for ds in dsets],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "\n",
    "def stitch_cont_features(dsets) -> np.ndarray:\n",
    "    return np.concatenate(\n",
    "        [ds.tensors[1].detach().cpu().numpy().astype(np.float32) for ds in dsets],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "\n",
    "Y_scaled = stitch_targets([train_ds, val_ds, test_ds])       \n",
    "X_scaled = stitch_cont_features([train_ds, val_ds, test_ds]) \n",
    "\n",
    "if Y_scaled.shape != Y.shape:\n",
    "    raise ValueError(f\"Y_scaled shape {Y_scaled.shape} != Y {Y.shape}\")\n",
    "if X_scaled.shape != X.shape:\n",
    "    raise ValueError(f\"X_scaled shape {X_scaled.shape} != X {X.shape}\")\n",
    "\n",
    "print(\"\\n=== After pooled scaling (train-only) ===\")\n",
    "print(\"Y_scaled:\", Y_scaled.shape, \"| finite:\", bool(np.isfinite(Y_scaled).all()))\n",
    "print(\"X_scaled:\", X_scaled.shape, \"| finite:\", bool(np.isfinite(X_scaled).all()))\n",
    "\n",
    "\n",
    "# Build TimeSeries lists (target + past cov + static)\n",
    "\n",
    "from typing import List as _ListTs \n",
    "series_all: _ListTs[TimeSeries] = []\n",
    "pcov_all: _ListTs[TimeSeries] = []   \n",
    "\n",
    "K_LAT = 6\n",
    "K_LON = 6\n",
    "if K_LAT * K_LON != N:\n",
    "    lat_pts = np.linspace(-5.0, 5.0, N).astype(np.float32)\n",
    "    lon_pts = np.zeros_like(lat_pts, dtype=np.float32)\n",
    "else:\n",
    "    lat_lin = np.linspace(18.0, 53.6, K_LAT, dtype=np.float32)\n",
    "    lon_lin = np.linspace(73.5, 134.8, K_LON, dtype=np.float32)\n",
    "    lat_grid, lon_grid = np.meshgrid(lat_lin, lon_lin, indexing=\"ij\")\n",
    "    lat_pts = lat_grid.reshape(-1).astype(np.float32)  \n",
    "    lon_pts = lon_grid.reshape(-1).astype(np.float32)  \n",
    "\n",
    "alt_pts = np.zeros_like(lat_pts, dtype=np.float32)  \n",
    "\n",
    "for j in range(N):\n",
    "    name = f\"grid_{j:03d}\"\n",
    "\n",
    "    ts = TimeSeries.from_times_and_values(\n",
    "        times=time_index,\n",
    "        values=Y_scaled[:, j:j + 1].astype(np.float32),\n",
    "        columns=[name],\n",
    "        freq=freq,\n",
    "    )\n",
    "\n",
    "    lat_s = float(lat_pts[j])\n",
    "    lon_s = float(lon_pts[j])\n",
    "    alt_s = float(alt_pts[j])\n",
    "    sc = pd.DataFrame(\n",
    "        {\"lat\": [lat_s], \"lon\": [lon_s], \"alt\": [alt_s]},\n",
    "        index=[name],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    ts = ts.with_static_covariates(sc)\n",
    "    series_all.append(ts)\n",
    "\n",
    "    pc = TimeSeries.from_times_and_values(\n",
    "        times=time_index,\n",
    "        values=X_scaled[:, j, :].astype(np.float32),  \n",
    "        columns=[f\"{name}_{c}\" for c in PAST_COV_COLS],\n",
    "        freq=freq,\n",
    "    ).with_static_covariates(sc)\n",
    "\n",
    "    pcov_all.append(pc)\n",
    "\n",
    "print(\"\\nBuilt TimeSeries:\")\n",
    "print(\"Targets:\", len(series_all))\n",
    "print(\"Past covs:\", len(pcov_all))\n",
    "\n",
    "\n",
    "# Internal validation (tail of train) for early stopping\n",
    "\n",
    "L, H = 56, 24\n",
    "INTERNAL_VAL_STEPS = 240  \n",
    "\n",
    "\n",
    "def slice_list(xs, a, b):\n",
    "    return [x[a:b] for x in xs]\n",
    "\n",
    "\n",
    "train_series = slice_list(series_all, 0, cut_train)\n",
    "train_pcov = slice_list(pcov_all, 0, cut_train)\n",
    "\n",
    "iv_start = max(0, cut_train - INTERNAL_VAL_STEPS)\n",
    "iv_end = cut_train\n",
    "\n",
    "min_needed = L + H\n",
    "if (iv_end - iv_start) < min_needed:\n",
    "    iv_start = max(0, iv_end - min_needed)\n",
    "\n",
    "val_series = slice_list(series_all, iv_start, iv_end)\n",
    "val_pcov = slice_list(pcov_all, iv_start, iv_end)\n",
    "\n",
    "print(\"\\n=== INTERNAL validation (for early stopping only) ===\")\n",
    "print(\"Train len:\", len(train_series[0]))\n",
    "print(\"IntVal idx:\", iv_start, \"->\", iv_end - 1, \"| len =\", len(val_series[0]))\n",
    "print(\"IntVal time:\", time_index[iv_start], \"->\", time_index[iv_end - 1])\n",
    "\n",
    "\n",
    "# Train TFT \n",
    "\n",
    "MODEL_NAME = f\"tft_synth_A_L{L}_H{H}_seed{GLOBAL_SEED}\"\n",
    "LOG_ROOT = (RUNS_DIR / f\"seed_{GLOBAL_SEED}\").resolve()\n",
    "LOG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tft = TFTModel(\n",
    "    input_chunk_length=L,\n",
    "    output_chunk_length=H,\n",
    "    n_epochs=30,\n",
    "    hidden_size=64,\n",
    "    num_attention_heads=4,\n",
    "    dropout=0.1,\n",
    "    batch_size=32,\n",
    "    optimizer_kwargs={\"lr\": 3e-4},\n",
    "    add_relative_index=True,\n",
    "    random_state=GLOBAL_SEED,\n",
    "    force_reset=True,\n",
    "    model_name=MODEL_NAME,\n",
    "    work_dir=str(CKPT_DIR),\n",
    "    save_checkpoints=True,\n",
    "    pl_trainer_kwargs={\n",
    "        \"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"devices\": 1,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"enable_model_summary\": False,\n",
    "        \"enable_checkpointing\": True,\n",
    "        \"callbacks\": [EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=6)],\n",
    "        \"logger\": CSVLogger(save_dir=str(LOG_ROOT), name=MODEL_NAME),\n",
    "        \"gradient_clip_val\": 1.0,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training TFT (Scenario A: linear world, with past cov) ===\")\n",
    "tft.fit(\n",
    "    series=train_series,\n",
    "    past_covariates=train_pcov,\n",
    "    val_series=val_series,\n",
    "    val_past_covariates=val_pcov,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Loading best checkpoint ===\")\n",
    "tft = TFTModel.load_from_checkpoint(model_name=MODEL_NAME, work_dir=str(CKPT_DIR), best=True)\n",
    "print(\"Loaded best checkpoint.\")\n",
    "\n",
    "# Rolling non-overlap over TEST \n",
    "\n",
    "def rolling_nonoverlap(model, series, pcov, start, end, L, H):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      yhat_roll: (W, H, N)\n",
    "      ytrue_roll:(W, H, N)\n",
    "      t0_list: list of window start indices\n",
    "    \"\"\"\n",
    "    yh, yt = [], []\n",
    "    t0_list = []\n",
    "    for t0 in range(start, end - H + 1, H):\n",
    "        if t0 < L:\n",
    "            continue\n",
    "\n",
    "        ctx_s = [s[:t0] for s in series]\n",
    "        ctx_p = [p[:t0] for p in pcov]\n",
    "\n",
    "        preds = model.predict(\n",
    "            n=H,\n",
    "            series=ctx_s,\n",
    "            past_covariates=ctx_p,\n",
    "            verbose=False,\n",
    "        )\n",
    "        preds = preds if isinstance(preds, list) else [preds]\n",
    "\n",
    "        yh.append(\n",
    "            np.stack([p.values(copy=False)[:, 0] for p in preds], axis=1).astype(np.float32)\n",
    "        )  \n",
    "        yt.append(\n",
    "            np.stack([s[t0:t0 + H].values(copy=False)[:, 0] for s in series], axis=1).astype(np.float32)\n",
    "        )  \n",
    "        t0_list.append(int(t0))\n",
    "\n",
    "    if len(yh) == 0:\n",
    "        raise RuntimeError(\"No rolling windows produced. Check start/end/L/H.\")\n",
    "    return np.stack(yh, axis=0), np.stack(yt, axis=0), t0_list\n",
    "\n",
    "\n",
    "test_start = cut_val\n",
    "start_ctx = max(test_start, L)\n",
    "yhat_roll, ytrue_roll, t0_list = rolling_nonoverlap(\n",
    "    tft, series_all, pcov_all, start_ctx, T_total, L, H\n",
    ")\n",
    "\n",
    "W = yhat_roll.shape[0]\n",
    "print(\"\\n=== Rolling non-overlap (POOLED, TEST) ===\")\n",
    "print(f\"windows={W} | step={H} | each window predicts H={H} | points={N}\")\n",
    "\n",
    "diff = yhat_roll - ytrue_roll\n",
    "rmse = float(np.sqrt(np.mean(diff ** 2)))\n",
    "mae = float(np.mean(np.abs(diff)))\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE :\", mae)\n",
    "\n",
    "#  P50 q-risk \n",
    "\n",
    "def qrisk(y_true, y_pred, q=0.5, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    e = y_true - y_pred\n",
    "    return float(\n",
    "        2.0 * np.sum(np.maximum(q * e, (q - 1) * e)) / (np.sum(np.abs(y_true)) + eps)\n",
    "    )\n",
    "\n",
    "\n",
    "yhat_f = yhat_roll.reshape(-1, N)\n",
    "ytrue_f = ytrue_roll.reshape(-1, N)\n",
    "\n",
    "print(\"\\n=== P50 q-risk (ROLLING, TEST) ===\")\n",
    "print(\"scaled  :\", qrisk(ytrue_f, yhat_f))\n",
    "\n",
    "ytrue_un = preprocessor.inverse_transform_targets(ytrue_f)\n",
    "yhat_un = preprocessor.inverse_transform_targets(yhat_f)\n",
    "print(\"unscaled:\", qrisk(ytrue_un, yhat_un))\n",
    "\n",
    "\n",
    "# Plot\n",
    "\n",
    "test_end = T_total\n",
    "test_len = test_end - test_start\n",
    "\n",
    "y_true_test = Y_scaled[test_start:test_end, :].astype(np.float32)  \n",
    "\n",
    "y_pred_test = np.full((test_len, N), np.nan, dtype=np.float32)\n",
    "for w, t0 in enumerate(t0_list):\n",
    "    a = t0 - test_start\n",
    "    b = a + H\n",
    "    if a < 0 or b > test_len:\n",
    "        continue\n",
    "    y_pred_test[a:b, :] = yhat_roll[w]\n",
    "\n",
    "w0 = 0\n",
    "t0_first = t0_list[w0]\n",
    "dates_first = time_index[t0_first:t0_first + H]\n",
    "dates_test = time_index[test_start:test_end]\n",
    "\n",
    "\n",
    "def plot_two_figs_all_points(out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for j in range(N):\n",
    "        point_id = j\n",
    "\n",
    "        y_true_H = ytrue_roll[w0][:, j]\n",
    "        y_pred_H = yhat_roll[w0][:, j]\n",
    "\n",
    "        plt.figure(figsize=(10, 4), dpi=140)\n",
    "        plt.plot(dates_first, y_true_H, \"-o\", linewidth=2, markersize=3, label=\"True\")\n",
    "        plt.plot(dates_first, y_pred_H, \"-o\", linewidth=2, markersize=3, label=\"Pred\")\n",
    "        plt.title(\n",
    "            f\"FIG1 Synthetic A | grid_{point_id:03d} | \"\n",
    "            f\"TEST first {H} steps | y (scaled)\"\n",
    "        )\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Scaled target\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        p1 = out_dir / f\"FIG1_grid{point_id:03d}_test_firstH{H}_t0{t0_first}.png\"\n",
    "        plt.savefig(p1)\n",
    "        plt.close()\n",
    "\n",
    "        y_true_all = y_true_test[:, j]\n",
    "        y_pred_all = y_pred_test[:, j]\n",
    "\n",
    "        plt.figure(figsize=(12, 4), dpi=140)\n",
    "        plt.plot(dates_test, y_true_all, \"-\", linewidth=1.8, label=\"True\")\n",
    "        plt.plot(\n",
    "            dates_test,\n",
    "            y_pred_all,\n",
    "            \"-\",\n",
    "            linewidth=1.8,\n",
    "            label=\"Pred (stitched rolling)\",\n",
    "        )\n",
    "        plt.axvline(time_index[test_start], linestyle=\"--\", linewidth=1, label=\"TEST start\")\n",
    "        plt.title(\n",
    "            f\"FIG2 Synthetic A | grid_{point_id:03d} | \"\n",
    "            f\"ALL TEST | y (scaled)\"\n",
    "        )\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Scaled target\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        p2 = out_dir / f\"FIG2_grid{point_id:03d}_test_all_stitched_step{H}.png\"\n",
    "        plt.savefig(p2)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Saved 2 figs/point for {N} points under: {out_dir}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== Plotting 2 figs per grid point (Scenario A) ===\")\n",
    "plot_two_figs_all_points(PLOTS_DIR)\n",
    "\n",
    "print(\"\\nAll done (Scenario A: linear world, TFT with past covariates).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
